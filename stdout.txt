Launching sbt from sbt/sbt-launch-0.13.7.jar
[0m[[0minfo[0m] [0mLoading project definition from /vagrant/project[0m
[0m[[0minfo[0m] [0mSet current project to mist (in build file:/vagrant/)[0m
[0m[[33mwarn[0m] [0mCredentials file /home/vagrant/.ivy2/.credentials does not exist[0m
[0m[[0minfo[0m] [0mUpdating {file:/vagrant/}mist...[0m
[0m[[0minfo[0m] [0mResolving org.scala-lang#scala-library;2.11.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-actor_2.11;2.4.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe#config;1.3.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang.modules#scala-java8-compat_2.11;0.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-cluster_2.11;2.4.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-remote_2.11;2.4.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-protobuf_2.11;2.4.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.netty#netty;3.10.3.Final ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.uncommons.maths#uncommons-maths;1.2.2a ...[0m
M[2K[0m[[0minfo[0m] [0mResolving ch.qos.logback#logback-classic;1.1.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving ch.qos.logback#logback-core;1.1.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.slf4j#slf4j-api;1.7.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang#scala-reflect;2.11.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-slf4j_2.11;2.4.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.slf4j#slf4j-api;1.7.12 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json4s#json4s-native_2.11;3.2.10 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json4s#json4s-core_2.11;3.2.10 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json4s#json4s-ast_2.11;3.2.10 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.thoughtworks.paranamer#paranamer;2.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang#scalap;2.11.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang#scala-compiler;2.11.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang.modules#scala-xml_2.11;1.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang.modules#scala-parser-combinators_2.11;1.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json4s#json4s-jackson_2.11;3.2.10 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.core#jackson-databind;2.3.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.core#jackson-annotations;2.3.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.core#jackson-core;2.3.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-http-core-experimental_2.11;2.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-parsing-experimental_2.11;2.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-stream-experimental_2.11;2.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe#ssl-config-akka_2.11;0.1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe#ssl-config-core_2.11;0.1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang.modules#scala-parser-combinators_2.11;1.0.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.reactivestreams#reactive-streams;1.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-http-experimental_2.11;2.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-http-spray-json-experimental_2.11;2.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.spray#spray-json_2.11;1.3.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.github.fge#json-schema-validator;2.2.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.code.findbugs#jsr305;3.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving joda-time#joda-time;2.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.googlecode.libphonenumber#libphonenumber;6.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.github.fge#json-schema-core;1.2.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.github.fge#uri-template;0.9 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.github.fge#msg-simple;1.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.github.fge#btf;1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.guava#guava;16.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.github.fge#jackson-coreutils;1.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.mozilla#rhino;1.7R4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.mail#mailapi;1.4.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.activation#activation;1.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.sf.jopt-simple#jopt-simple;4.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scalactic#scalactic_2.11;2.2.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang#scala-reflect;2.11.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.mapdb#mapdb;3.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.jetbrains.kotlin#kotlin-stdlib;1.0.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.jetbrains.kotlin#kotlin-runtime;1.0.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.eclipse.collections#eclipse-collections-api;[7.0.0,7.20.0) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.eclipse.collections#eclipse-collections-api;[7.0.0,7.20.0) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.jcip#jcip-annotations;1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.eclipse.collections#eclipse-collections;[7.0.0,7.20.0) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.eclipse.collections#eclipse-collections;[7.0.0,7.20.0) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.eclipse.collections#eclipse-collections-forkjoin;[7.0.0,7.20.0) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.eclipse.collections#eclipse-collections-forkjoin;[7.0.0,7.20.0) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.guava#guava;[15.0,19.20) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.guava#guava;[15.0,19.20) ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.jpountz.lz4#lz4;1.3.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.mapdb#elsa;3.0.0-M5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.eclipse.paho#org.eclipse.paho.client.mqttv3;1.1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scalatest#scalatest_2.11;2.2.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang.modules#scala-xml_2.11;1.0.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.typesafe.akka#akka-testkit_2.11;2.3.12 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scalamock#scalamock-scalatest-support_2.11;3.2.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scalamock#scalamock-core_2.11;3.2.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scoverage#scalac-scoverage-runtime_2.11;1.1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang#scala-library;2.11.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scoverage#scalac-scoverage-plugin_2.11;1.1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-core_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.avro#avro-mapred;1.7.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.avro#avro-ipc;1.7.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.avro#avro;1.7.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.codehaus.jackson#jackson-core-asl;1.9.13 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.codehaus.jackson#jackson-mapper-asl;1.9.13 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.thoughtworks.paranamer#paranamer;2.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.xerial.snappy#snappy-java;1.0.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.commons#commons-compress;1.4.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.tukaani#xz;1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.twitter#chill_2.11;0.8.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.twitter#chill-java;0.8.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.esotericsoftware#kryo-shaded;3.0.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.esotericsoftware#minlog;1.3.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.objenesis#objenesis;2.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.xbean#xbean-asm5-shaded;4.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-common;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-annotations;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.guava#guava;11.0.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.code.findbugs#jsr305;1.3.9 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-cli#commons-cli;1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.commons#commons-math;2.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving xmlenc#xmlenc;0.52 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-httpclient#commons-httpclient;3.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-codec#commons-codec;1.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-io#commons-io;2.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving log4j#log4j;1.2.17 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-lang#commons-lang;2.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-configuration#commons-configuration;1.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-collections#commons-collections;3.2.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-digester#commons-digester;1.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-beanutils#commons-beanutils;1.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-beanutils#commons-beanutils-core;1.8.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.protobuf#protobuf-java;2.5.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-auth;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-hdfs;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.mortbay.jetty#jetty-util;6.1.26 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-app;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-app;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-common;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-common;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-common;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-common;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-api;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-api;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.slf4j#slf4j-log4j12;1.7.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.inject#guice;3.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.inject#javax.inject;1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving aopalliance#aopalliance;1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.sonatype.sisu.inject#cglib;2.2.1-v20090111 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-core;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-core;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-server-common;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-server-common;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-server;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-server;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-shuffle;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-shuffle;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-jobclient;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client-jobclient;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-mapreduce-client;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-server-nodemanager;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-server-nodemanager;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.hadoop#hadoop-yarn-server;2.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-launcher_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-tags_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.spark-project.spark#unused;1.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-network-common_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.netty#netty-all;4.0.29.Final ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-network-shuffle_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.fusesource.leveldbjni#leveldbjni-all;1.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.core#jackson-databind;2.6.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.core#jackson-core;2.6.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.core#jackson-annotations;2.6.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-unsafe_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.java.dev.jets3t#jets3t;0.7.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.curator#curator-recipes;2.4.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.curator#curator-framework;2.4.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.curator#curator-client;2.4.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.zookeeper#zookeeper;3.4.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.google.guava#guava;14.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.servlet#javax.servlet-api;3.1.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.commons#commons-lang3;3.3.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.commons#commons-math3;3.4.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.slf4j#slf4j-api;1.7.16 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.slf4j#jul-to-slf4j;1.7.16 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.slf4j#jcl-over-slf4j;1.7.16 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.slf4j#slf4j-log4j12;1.7.16 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.ning#compress-lzf;1.0.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.xerial.snappy#snappy-java;1.1.2.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.roaringbitmap#RoaringBitmap;0.5.11 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-net#commons-net;2.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json4s#json4s-jackson_2.11;3.2.11 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json4s#json4s-core_2.11;3.2.11 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json4s#json4s-ast_2.11;3.2.11 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.jersey.core#jersey-client;2.22.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.ws.rs#javax.ws.rs-api;2.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.jersey.core#jersey-common;2.22.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.annotation#javax.annotation-api;1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.jersey.bundles.repackaged#jersey-guava;2.22.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.hk2#hk2-api;2.4.0-b34 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.hk2#hk2-utils;2.4.0-b34 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.hk2.external#aopalliance-repackaged;2.4.0-b34 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.hk2.external#javax.inject;2.4.0-b34 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.hk2#hk2-locator;2.4.0-b34 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.javassist#javassist;3.18.1-GA ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.hk2#osgi-resource-locator;1.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.jersey.core#jersey-server;2.22.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.jersey.media#jersey-media-jaxb;2.22.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.validation#validation-api;1.1.0.Final ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.jersey.containers#jersey-container-servlet;2.22.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.glassfish.jersey.containers#jersey-container-servlet-core;2.22.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.mesos#mesos;0.21.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.netty#netty;3.8.0.Final ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.clearspring.analytics#stream;2.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.dropwizard.metrics#metrics-core;3.1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.dropwizard.metrics#metrics-jvm;3.1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.dropwizard.metrics#metrics-json;3.1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving io.dropwizard.metrics#metrics-graphite;3.1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.module#jackson-module-scala_2.11;2.6.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.fasterxml.jackson.module#jackson-module-paranamer;2.6.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.ivy#ivy;2.4.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving oro#oro;2.0.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.razorvine#pyrolite;4.9 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.sf.py4j#py4j;0.10.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-sql_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.univocity#univocity-parsers;2.1.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-sketch_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-catalyst_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang#scala-reflect;2.11.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.codehaus.janino#janino;2.7.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.codehaus.janino#commons-compiler;2.7.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.antlr#antlr4-runtime;4.5.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-codec#commons-codec;1.10 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.parquet#parquet-column;1.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.parquet#parquet-common;1.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.parquet#parquet-encoding;1.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.parquet#parquet-generator;1.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.parquet#parquet-hadoop;1.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.parquet#parquet-format;2.3.0-incubating ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.parquet#parquet-jackson;1.7.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.spark#spark-hive_2.11;2.0.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.twitter#parquet-hadoop-bundle;1.6.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.spark-project.hive#hive-exec;1.2.1.spark2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-io#commons-io;2.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-lang#commons-lang;2.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javolution#javolution;5.5.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving log4j#apache-log4j-extras;1.2.17 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.antlr#antlr-runtime;3.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.antlr#stringtemplate;3.2.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving antlr#antlr;2.7.7 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.antlr#ST4;4.0.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.jodd#jodd-core;3.5.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.datanucleus#datanucleus-core;3.2.10 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.calcite#calcite-avatica;1.2.0-incubating ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.googlecode.javaewah#JavaEWAH;0.3.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.iq80.snappy#snappy;0.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.json#json;20090211 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving stax#stax-api;1.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.sf.opencsv#opencsv;2.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving jline#jline;2.12 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.spark-project.hive#hive-metastore;1.2.1.spark2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving com.jolbox#bonecp;0.8.0.RELEASE ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-logging#commons-logging;1.1.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.derby#derby;10.10.2.0 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.datanucleus#datanucleus-api-jdo;3.2.6 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.datanucleus#datanucleus-rdbms;3.2.9 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-pool#commons-pool;1.5.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-dbcp#commons-dbcp;1.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.jdo#jdo-api;3.0.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving javax.transaction#jta;1.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.calcite#calcite-core;1.2.0-incubating ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.calcite#calcite-linq4j;1.2.0-incubating ...[0m
M[2K[0m[[0minfo[0m] [0mResolving net.hydromatic#eigenbase-properties;1.1.5 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.httpcomponents#httpclient;4.5.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.httpcomponents#httpcore;4.4.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-logging#commons-logging;1.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving joda-time#joda-time;2.9.3 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.thrift#libthrift;0.9.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.apache.thrift#libfb303;0.9.2 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving commons-net#commons-net;3.1 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang#scala-compiler;2.11.8 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving org.scala-lang.modules#scala-xml_2.11;1.0.4 ...[0m
M[2K[0m[[0minfo[0m] [0mResolving jline#jline;2.12.1 ...[0m
[0m[[0minfo[0m] [0mdownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.12/slf4j-api-1.7.12.jar ...[0m
[0m[[0minfo[0m] [0m	[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.12!slf4j-api.jar (859ms)[0m
[0m[[0minfo[0m] [0mdownloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.1/scala-parser-combinators_2.11-1.0.1.jar ...[0m
[0m[[0minfo[0m] [0m	[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.0.1!scala-parser-combinators_2.11.jar(bundle) (802ms)[0m
[0m[[0minfo[0m] [0mDone updating.[0m
[0m[[33mwarn[0m] [0mThere may be incompatibilities among your library dependencies.[0m
[0m[[33mwarn[0m] [0mHere are some of the libraries that were evicted:[0m
[0m[[33mwarn[0m] [0m	* com.typesafe.akka:akka-actor_2.11:(2.3.12, 2.4.1) -> 2.4.7[0m
[0m[[33mwarn[0m] [0mRun 'evicted' to see detailed eviction warnings[0m
[0m[[0minfo[0m] [0mCompiling 29 Scala sources to /vagrant/target/scala-2.11/classes...[0m
[0m[[33mwarn[0m] [0m/vagrant/src/main/scala/io/hydrosphere/mist/MistConfig.scala:78: This catches all Throwables. If this is really intended, use `case _ : Throwable` to clear this warning.[0m
[0m[[33mwarn[0m] [0m        case _ => false[0m
[0m[[33mwarn[0m] [0m             ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/main/scala/io/hydrosphere/mist/logger/Logger.scala:3: imported `Logger' is permanently hidden by definition of trait Logger in package mist[0m
[0m[[33mwarn[0m] [0mimport org.slf4j.Logger[0m
[0m[[33mwarn[0m] [0m                 ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/main/scala/io/hydrosphere/mist/master/HTTPService.scala:25: non-variable type argument String in type pattern scala.collection.immutable.Map[String,_] (the underlying of Map[String,_]) is unchecked since it is eliminated by erasure[0m
[0m[[33mwarn[0m] [0m      case map: Map[String, _] => mapFormat[String, Any] write map[0m
[0m[[33mwarn[0m] [0m                ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/main/scala/io/hydrosphere/mist/master/HTTPService.scala:71: non-variable type argument Map[String,Any] in type pattern scala.util.Either[Map[String,Any],String] (the underlying of Either[Map[String,Any],String]) is unchecked since it is eliminated by erasure[0m
[0m[[33mwarn[0m] [0m              case result: Either[Map[String, Any], String] =>[0m
[0m[[33mwarn[0m] [0m                           ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/main/scala/io/hydrosphere/mist/master/mqtt/MQTTService.scala:52: non-variable type argument Map[String,Any] in type pattern scala.util.Either[Map[String,Any],String] (the underlying of Either[Map[String,Any],String]) is unchecked since it is eliminated by erasure[0m
[0m[[33mwarn[0m] [0m            case result: Either[Map[String, Any], String] =>[0m
[0m[[33mwarn[0m] [0m                         ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/main/scala/io/hydrosphere/mist/worker/ContextNode.scala:59: non-variable type argument Map[String,Any] in type pattern scala.util.Either[Map[String,Any],String] (the underlying of Either[Map[String,Any],String]) is unchecked since it is eliminated by erasure[0m
[0m[[33mwarn[0m] [0m          case Success(result: Either[Map[String, Any], String]) => originalSender ! result[0m
[0m[[33mwarn[0m] [0m                               ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/main/scala/io/hydrosphere/mist/Master.scala:45: match may not be exhaustive.[0m
[0m[[33mwarn[0m] [0mIt would fail on the following input: false[0m
[0m[[33mwarn[0m] [0m  MistConfig.Recovery.recoveryOn match {[0m
[0m[[33mwarn[0m] [0m                      ^[0m
[0m[[33mwarn[0m] [0mthere was one deprecation warning; re-run with -deprecation for details[0m
[0m[[33mwarn[0m] [0m8 warnings found[0m
[0m[[0minfo[0m] [0mCompiling 8 Scala sources to /vagrant/target/scala-2.11/test-classes...[0m
[0m[[33mwarn[0m] [0m/vagrant/src/test/scala/io/hydrosphere/mist/JobRecoveryTest.scala:8: imported `MistConfig' is permanently hidden by definition of object MistConfig in package mist[0m
[0m[[33mwarn[0m] [0mimport io.hydrosphere.mist.{MistConfig, TestConfig}[0m
[0m[[33mwarn[0m] [0m                            ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/test/scala/io/hydrosphere/mist/JobRecoveryTest.scala:8: imported `TestConfig' is permanently hidden by definition of object TestConfig in package mist[0m
[0m[[33mwarn[0m] [0mimport io.hydrosphere.mist.{MistConfig, TestConfig}[0m
[0m[[33mwarn[0m] [0m                                        ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/test/scala/io/hydrosphere/mist/TestMqttSubPub.scala:16: A try without a catch or finally is equivalent to putting its body in a block; no exceptions are handled.[0m
[0m[[33mwarn[0m] [0m    try {[0m
[0m[[33mwarn[0m] [0m    ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/test/scala/io/hydrosphere/mist/WorkerTest.scala:163: non-variable type argument Map[String,Any] in type pattern scala.util.Either[Map[String,Any],String] (the underlying of Either[Map[String,Any],String]) is unchecked since it is eliminated by erasure[0m
[0m[[33mwarn[0m] [0m            case result: Either[Map[String, Any], String] =>[0m
[0m[[33mwarn[0m] [0m                         ^[0m
[0m[[33mwarn[0m] [0m/vagrant/src/test/scala/io/hydrosphere/mist/JobRecoveryTest.scala:71: match may not be exhaustive.[0m
[0m[[33mwarn[0m] [0mIt would fail on the following input: false[0m
[0m[[33mwarn[0m] [0m      MistConfig.Recovery.recoveryOn match {[0m
[0m[[33mwarn[0m] [0m                          ^[0m
[0m[[33mwarn[0m] [0mthere were 92 feature warnings; re-run with -feature for details[0m
[0m[[33mwarn[0m] [0m6 warnings found[0m
14:10:53,839 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.groovy]
14:10:53,842 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]
14:10:53,845 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Found resource [logback.xml] at [file:/vagrant/target/scala-2.11/classes/logback.xml]
14:10:53,947 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - debug attribute not set
14:10:53,953 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]
14:10:53,972 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [sout]
14:10:54,106 |-WARN in ch.qos.logback.core.ConsoleAppender[sout] - This appender no longer admits a layout as a sub-component, set an encoder instead.
14:10:54,106 |-WARN in ch.qos.logback.core.ConsoleAppender[sout] - To ensure compatibility, wrapping your layout in LayoutWrappingEncoder.
14:10:54,106 |-WARN in ch.qos.logback.core.ConsoleAppender[sout] - See also http://logback.qos.ch/codes.html#layoutInsteadOfEncoder for details
14:10:54,110 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.FileAppender]
14:10:54,112 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [log4j]
14:10:54,126 |-WARN in ch.qos.logback.core.FileAppender[log4j] - This appender no longer admits a layout as a sub-component, set an encoder instead.
14:10:54,126 |-WARN in ch.qos.logback.core.FileAppender[log4j] - To ensure compatibility, wrapping your layout in LayoutWrappingEncoder.
14:10:54,126 |-WARN in ch.qos.logback.core.FileAppender[log4j] - See also http://logback.qos.ch/codes.html#layoutInsteadOfEncoder for details
14:10:54,126 |-INFO in ch.qos.logback.core.FileAppender[log4j] - File property is set to [./logs/log_log4j.log]
14:10:54,129 |-INFO in ch.qos.logback.classic.joran.action.RootLoggerAction - Setting level of ROOT logger to INFO
14:10:54,130 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [sout] to Logger[ROOT]
14:10:54,132 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [log4j] to Logger[ROOT]
14:10:54,132 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - End of configuration.
14:10:54,133 |-INFO in ch.qos.logback.classic.joran.JoranConfigurator@13b2dc3a - Registering current configuration as safe fallback point

14:10:54.530 [JobRecoveryTestActorSystem-akka.actor.default-dispatcher-5] INFO  i.h.mist.master.mqtt.MqttPubSub - connecting to tcp://192.168.10.33:1883..
14:10:56.502 [JobRecoveryTestActorSystem-akka.actor.default-dispatcher-2] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 3 get from MapDb
14:10:56.507 [JobRecoveryTestActorSystem-akka.actor.default-dispatcher-2] INFO  i.h.mist.master.JobRecovery - 3 loaded from MapDb 
14:10:56.607 [JobRecoveryTestActorSystem-akka.actor.default-dispatcher-2] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
14:10:57.622 [JobRecoveryTestActorSystem-akka.actor.default-dispatcher-2] INFO  i.h.mist.master.JobRecovery - send {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}
14:10:57.627 [JobRecoveryTestActorSystem-akka.actor.default-dispatcher-2] INFO  i.h.mist.master.JobRecovery - send {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}
14:10:57.628 [JobRecoveryTestActorSystem-akka.actor.default-dispatcher-3] INFO  i.h.mist.master.JobRecovery - send {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}
[0m[[0minfo[0m] [0m[32mJobRecoveryTest:[0m[0m
[0m[[0minfo[0m] [0m[32mRecovery 3 jobs[0m[0m
[0m[[0minfo[0m] [0m[32m- must All recovered ok[0m[0m
[0m[[0minfo[0m] [0m[32mAnyJsonTest:[0m[0m
[0m[[0minfo[0m] [0m[32m- AnyJsonFormat read[0m[0m
[0m[[0minfo[0m] [0m[32m- AnyJsonFormat write[0m[0m
[0m[[0minfo[0m] [0m[32m- AnyJsonFormat serializationError[0m[0m
[0m[[0minfo[0m] [0m[32m- AnyJsonFormat deserilalizationError[0m[0m
[0m[[0minfo[0m] [0m[32m- Constants Errors and Actors[0m[0m
[DEBUG] [08/18/2016 14:11:04.214] [pool-4-thread-2] [EventStream] StandardOutLogger started
14:11:04.304 [mist-akka.actor.default-dispatcher-5] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[DEBUG] [08/18/2016 14:11:04.305] [pool-4-thread-2] [EventStream(akka://mist)] logger log1-Slf4jLogger started
[DEBUG] [08/18/2016 14:11:04.306] [pool-4-thread-2] [EventStream(akka://mist)] Default Loggers started
14:11:04.492 [mist-akka.actor.default-dispatcher-5] INFO  akka.remote.Remoting - Starting remoting
14:11:04.862 [mist-akka.actor.default-dispatcher-6] INFO  akka.remote.Remoting - Remoting started; listening on addresses :[akka.tcp://mist@127.0.0.1:2551]
14:11:04.921 [mist-akka.actor.default-dispatcher-6] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Starting up...
14:11:05.111 [mist-akka.actor.default-dispatcher-6] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Registered cluster JMX MBean [akka:type=Cluster]
14:11:05.113 [mist-akka.actor.default-dispatcher-6] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Started up successfully
14:11:05.159 [mist-akka.actor.default-dispatcher-3] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Metrics will be retreived from MBeans, and may be incorrect on some platforms. To increase metric accuracy add the 'sigar.jar' to the classpath and the appropriate platform-specific native libary to 'java.library.path'. Reason: java.lang.ClassNotFoundException: org.hyperic.sigar.Sigar
[DEBUG] [08/18/2016 14:11:05.192] [pool-4-thread-2] [EventStream] StandardOutLogger started
14:11:05.211 [mist-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[DEBUG] [08/18/2016 14:11:05.212] [pool-4-thread-2] [EventStream(akka://mist)] logger log1-Slf4jLogger started
[DEBUG] [08/18/2016 14:11:05.213] [pool-4-thread-2] [EventStream(akka://mist)] Default Loggers started
14:11:05.220 [mist-akka.actor.default-dispatcher-4] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Metrics collection has started successfully
14:11:05.228 [mist-akka.actor.default-dispatcher-3] INFO  akka.remote.Remoting - Starting remoting
14:11:05.253 [mist-akka.actor.default-dispatcher-10] INFO  akka.remote.Remoting - Remoting started; listening on addresses :[akka.tcp://mist@127.0.0.1:41022]
14:11:05.257 [mist-akka.actor.default-dispatcher-10] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:41022] - Starting up...
14:11:05.259 [mist-akka.actor.default-dispatcher-2] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:41022] - Started up successfully
14:11:05.283 [mist-akka.actor.default-dispatcher-5] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:41022] - Metrics will be retreived from MBeans, and may be incorrect on some platforms. To increase metric accuracy add the 'sigar.jar' to the classpath and the appropriate platform-specific native libary to 'java.library.path'. Reason: java.lang.ClassNotFoundException: org.hyperic.sigar.Sigar
14:11:05.287 [mist-akka.actor.default-dispatcher-5] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:41022] - Metrics collection has started successfully
14:11:05.385 [mist-akka.actor.default-dispatcher-4] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Node [akka.tcp://mist@127.0.0.1:2551] is JOINING, roles []
14:11:05.460 [mist-akka.actor.default-dispatcher-5] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Leader is moving node [akka.tcp://mist@127.0.0.1:2551] to [Up]
14:11:06.126 [mist-akka.actor.default-dispatcher-2] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Node [akka.tcp://mist@127.0.0.1:41022] is JOINING, roles []
14:11:06.250 [mist-akka.actor.default-dispatcher-18] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Leader is moving node [akka.tcp://mist@127.0.0.1:41022] to [Up]
14:11:06.559 [mist-akka.actor.default-dispatcher-4] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:41022] - Welcome from [akka.tcp://mist@127.0.0.1:2551]
14:11:07.368 [mist-akka.actor.default-dispatcher-3] INFO  i.h.mist.master.mqtt.MqttPubSub - connecting to tcp://192.168.10.33:1883..
14:11:07.461 [MQTT Call: paho781399266383756] INFO  i.h.m.m.m.MqttPubSub$SubscribeListener$ - subscribed to [foo]
14:11:17.591 [mist-akka.actor.default-dispatcher-16] INFO  i.h.mist.master.mqtt.MqttPubSub - connecting to tcp://192.168.10.33:1883..
(TestActor: catch Up  akka.tcp://mist@127.0.0.1:2551,akka.tcp://mist@127.0.0.1:41022)
(TestActor workerUp:,false)
(TestActor: catch Up  akka.tcp://mist@127.0.0.1:41022,akka.tcp://mist@127.0.0.1:41022)
(TestActor workerUp:,true)
14:11:17.646 [mist-akka.actor.default-dispatcher-9] WARN  a.s.Serialization(akka://mist) - Using the default Java serializer for class [io.hydrosphere.mist.Messages$WorkerDidStart] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'
14:11:17.680 [mist-akka.actor.default-dispatcher-16] INFO  i.h.mist.master.WorkerManager - Worker `foo` did start on akka.tcp://mist@127.0.0.1:41022
(workerUp status,true)
14:11:22.753 [mist-akka.actor.default-dispatcher-9] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(src/test/resources/mist_examples_2.11-0.0.2.jar),None,Some(SimpleContext$),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:22.955 [pool-7-thread-1] INFO  org.apache.spark.SparkContext - Running Spark version 2.0.0
14:11:23.579 [pool-7-thread-1] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14:11:23.897 [pool-7-thread-1] INFO  org.apache.spark.SecurityManager - Changing view acls to: vagrant
14:11:23.899 [pool-7-thread-1] INFO  org.apache.spark.SecurityManager - Changing modify acls to: vagrant
14:11:23.900 [pool-7-thread-1] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
14:11:23.902 [pool-7-thread-1] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
14:11:23.903 [pool-7-thread-1] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(vagrant); groups with view permissions: Set(); users  with modify permissions: Set(vagrant); groups with modify permissions: Set()
14:11:24.561 [pool-7-thread-1] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 34878.
14:11:24.617 [pool-7-thread-1] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
14:11:24.664 [pool-7-thread-1] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
14:11:24.723 [pool-7-thread-1] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-6ef0cb4c-6907-4454-a4ea-d4b1c5f68dde
14:11:24.756 [pool-7-thread-1] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 632.0 MB
14:11:24.849 [pool-7-thread-1] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
14:11:25.121 [pool-7-thread-1] INFO  org.spark_project.jetty.util.log - Logging initialized @184054ms
14:11:25.469 [pool-7-thread-1] INFO  o.spark_project.jetty.server.Server - jetty-9.2.z-SNAPSHOT
14:11:25.530 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68fd5b23{/jobs,null,AVAILABLE}
14:11:25.531 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2e446e{/jobs/json,null,AVAILABLE}
14:11:25.533 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5106d46b{/jobs/job,null,AVAILABLE}
14:11:25.533 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5554bd6{/jobs/job/json,null,AVAILABLE}
14:11:25.534 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7732c1c{/stages,null,AVAILABLE}
14:11:25.535 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@186115a8{/stages/json,null,AVAILABLE}
14:11:25.543 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7fa33f43{/stages/stage,null,AVAILABLE}
14:11:25.545 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5007c09a{/stages/stage/json,null,AVAILABLE}
14:11:25.547 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e8a054a{/stages/pool,null,AVAILABLE}
14:11:25.550 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e569566{/stages/pool/json,null,AVAILABLE}
14:11:25.554 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3869e0c8{/storage,null,AVAILABLE}
14:11:25.555 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5684f44f{/storage/json,null,AVAILABLE}
14:11:25.556 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@483a1b80{/storage/rdd,null,AVAILABLE}
14:11:25.559 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@222d5a44{/storage/rdd/json,null,AVAILABLE}
14:11:25.561 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@55359169{/environment,null,AVAILABLE}
14:11:25.563 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24e2310c{/environment/json,null,AVAILABLE}
14:11:25.567 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c1cd80b{/executors,null,AVAILABLE}
14:11:25.571 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@74ab4ab6{/executors/json,null,AVAILABLE}
14:11:25.574 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@dec3393{/executors/threadDump,null,AVAILABLE}
14:11:25.574 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38330973{/executors/threadDump/json,null,AVAILABLE}
14:11:25.715 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62b542fe{/static,null,AVAILABLE}
14:11:25.716 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4ce55ae4{/,null,AVAILABLE}
14:11:25.727 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20f98840{/api,null,AVAILABLE}
14:11:25.731 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a165bd{/stages/stage/kill,null,AVAILABLE}
14:11:25.745 [pool-7-thread-1] INFO  o.s.jetty.server.ServerConnector - Started ServerConnector@7ae8d7cd{HTTP/1.1}{0.0.0.0:4040}
14:11:25.745 [pool-7-thread-1] INFO  o.spark_project.jetty.server.Server - Started @184679ms
14:11:25.746 [pool-7-thread-1] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
14:11:25.753 [pool-7-thread-1] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
14:11:26.105 [pool-7-thread-1] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
14:11:26.155 [pool-7-thread-1] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44508.
14:11:26.156 [pool-7-thread-1] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on 10.0.2.15:44508
14:11:26.162 [pool-7-thread-1] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 10.0.2.15, 44508)
14:11:26.174 [dispatcher-event-loop-0] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager 10.0.2.15:44508 with 632.0 MB RAM, BlockManagerId(driver, 10.0.2.15, 44508)
14:11:26.195 [pool-7-thread-1] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 10.0.2.15, 44508)
14:11:26.233 [pool-7-thread-1] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@159b9d4e{/metrics/json,null,AVAILABLE}
14:11:26.383 [pool-7-thread-1] INFO  org.apache.spark.SparkContext - Added JAR src/test/resources/mist_examples_2.11-0.0.2.jar at spark://10.0.2.15:34878/jars/mist_examples_2.11-0.0.2.jar with timestamp 1471529486382
14:11:26.391 [mist-akka.actor.default-dispatcher-6] WARN  a.s.Serialization(akka://mist) - Using the default Java serializer for class [io.hydrosphere.mist.Messages$AddJobToRecovery] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'
14:11:26.397 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - foo#b1a34220-c5fe-432c-904f-8be46ba91dde is running
14:11:26.494 [mist-akka.actor.default-dispatcher-17] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - b1a34220-c5fe-432c-904f-8be46ba91dde saved in MapDb
14:11:27.630 [pool-7-thread-1] INFO  org.apache.spark.SparkContext - Starting job: collect at SimpleContext.scala:15
14:11:27.693 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 0 (collect at SimpleContext.scala:15) with 1 output partitions
14:11:27.698 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (collect at SimpleContext.scala:15)
14:11:27.699 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
14:11:27.702 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
14:11:27.723 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SimpleContext.scala:15), which has no missing parents
14:11:27.928 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 1960.0 B, free 632.0 MB)
14:11:28.018 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1276.0 B, free 632.0 MB)
14:11:28.022 [dispatcher-event-loop-0] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 10.0.2.15:44508 (size: 1276.0 B, free: 632.0 MB)
14:11:28.043 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1012
14:11:28.056 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SimpleContext.scala:15)
14:11:28.061 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
14:11:28.239 [dispatcher-event-loop-1] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 6091 bytes)
14:11:28.304 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
14:11:28.326 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Fetching spark://10.0.2.15:34878/jars/mist_examples_2.11-0.0.2.jar with timestamp 1471529486382
14:11:28.489 [Executor task launch worker-0] INFO  o.a.s.n.c.TransportClientFactory - Successfully created connection to /10.0.2.15:34878 after 69 ms (0 ms spent in bootstraps)
14:11:28.547 [Executor task launch worker-0] INFO  org.apache.spark.util.Utils - Fetching spark://10.0.2.15:34878/jars/mist_examples_2.11-0.0.2.jar to /tmp/spark-ecfb90b3-5ac5-44fc-a32a-9a213ce9448d/userFiles-a0ba0d09-c47d-4b1f-b1b9-1da5d7c49601/fetchFileTemp2080355457015086472.tmp
14:11:28.712 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Adding file:/tmp/spark-ecfb90b3-5ac5-44fc-a32a-9a213ce9448d/userFiles-a0ba0d09-c47d-4b1f-b1b9-1da5d7c49601/mist_examples_2.11-0.0.2.jar to class loader
14:11:28.821 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1627 bytes result sent to driver
14:11:28.855 [task-result-getter-0] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 731 ms on localhost (1/1)
14:11:28.870 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 0 (collect at SimpleContext.scala:15) finished in 0.786 s
14:11:28.874 [task-result-getter-0] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
14:11:28.895 [pool-7-thread-1] INFO  o.a.spark.scheduler.DAGScheduler - Job 0 finished: collect at SimpleContext.scala:15, took 1.264224 s
14:11:28.910 [mist-akka.actor.default-dispatcher-6] WARN  a.s.Serialization(akka://mist) - Using the default Java serializer for class [io.hydrosphere.mist.Messages$RemoveJobFromRecovery] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'
14:11:28.949 [mist-akka.actor.default-dispatcher-17] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - b1a34220-c5fe-432c-904f-8be46ba91dde removed from MapDb
HttpResponse(400 Bad Request,List(Server: akka-http/2.4.7, Date: Thu, 18 Aug 2016 14:11:30 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,The request content was malformed:
Unexpected character 'b' at input index 0 (line 1, position 1), expected JSON Value:
bad request
^
),HttpProtocol(HTTP/1.1))
14:11:30.546 [mist-akka.actor.default-dispatcher-4] INFO  i.h.mist.workerManagerTestActor - Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0))
14:11:30.561 [mist-akka.actor.default-dispatcher-2] WARN  a.s.Serialization(akka://mist) - Using the default Java serializer for class [io.hydrosphere.mist.jobs.JobConfiguration] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'
14:11:30.590 [mist-akka.actor.default-dispatcher-3] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(/target/scala-2.10/bad.jar),None,Some(BadclassName$),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:30.625 [pool-7-thread-2] ERROR io.hydrosphere.mist.jobs.Job$ - /target/scala-2.10/bad.jar (No such file or directory)
java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[na:1.8.0_101]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:93) ~[na:1.8.0_101]
	at java.io.FileReader.<init>(FileReader.java:58) ~[na:1.8.0_101]
	at io.hydrosphere.mist.jobs.Job$.apply(Job.scala:40) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:46) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:45) [classes/:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [scala-library-2.11.8.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:11:30.627 [ForkJoinPool-6-worker-3] ERROR io.hydrosphere.mist.jobs.Job$ - /target/scala-2.10/bad.jar (No such file or directory)
java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[na:1.8.0_101]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:93) ~[na:1.8.0_101]
	at java.io.FileReader.<init>(FileReader.java:58) ~[na:1.8.0_101]
	at io.hydrosphere.mist.jobs.Job$.apply(Job.scala:40) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$applyOrElse$2.applyOrElse(ContextNode.scala:56) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$applyOrElse$2.applyOrElse(ContextNode.scala:54) [classes/:na]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) [scala-library-2.11.8.jar:na]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.8.jar:na]
14:11:30.630 [mist-akka.actor.default-dispatcher-10] WARN  a.s.Serialization(akka://mist) - Using the default Java serializer for class [scala.util.Right] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'
14:11:31.157 [dispatcher-event-loop-0] INFO  o.a.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 10.0.2.15:44508 in memory (size: 1276.0 B, free: 632.0 MB)
HttpResponse(200 OK,List(Server: akka-http/2.4.7, Date: Thu, 18 Aug 2016 14:11:31 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{"success":false,"payload":{},"errors":["java.lang.Exception: java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)"],"request":{"jarPath":"/target/scala-2.10/bad.jar","className":"BadclassName$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}),HttpProtocol(HTTP/1.1))
HttpResponse(400 Bad Request,List(Server: akka-http/2.4.7, Date: Thu, 18 Aug 2016 14:11:31 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,The request content was malformed:
Unexpected character '.' at input index 0 (line 1, position 1), expected JSON Value:
...{"TestErrorBadJson":"TestErrorBadJson$"}
^
),HttpProtocol(HTTP/1.1))
14:11:31.329 [mist-akka.actor.default-dispatcher-4] INFO  i.h.mist.workerManagerTestActor - Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0))
14:11:31.333 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(src/test/resources/mist_examples_2.11-0.0.2.jar),None,Some(noDoStuffMethodError),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
HttpResponse(200 OK,List(Server: akka-http/2.4.7, Date: Thu, 18 Aug 2016 14:11:31 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{"success":false,"payload":{},"errors":["java.lang.NoSuchFieldException: MODULE$"],"request":{"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"noDoStuffMethodError","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}),HttpProtocol(HTTP/1.1))
14:11:31.477 [mist-akka.actor.default-dispatcher-5] INFO  i.h.mist.workerManagerTestActor - Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0))
14:11:31.487 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(src/test/resources/mist_examples_2.11-0.0.2.jar),None,Some(SimpleContext$),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:31.510 [mist-akka.actor.default-dispatcher-5] INFO  i.h.mist.worker.ContextNode - foo#54d0759f-9ae9-4d1e-a983-8199bf78c05e is running
14:11:31.562 [pool-7-thread-4] INFO  org.apache.spark.SparkContext - Starting job: collect at SimpleContext.scala:15
14:11:31.563 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 1 (collect at SimpleContext.scala:15) with 1 output partitions
14:11:31.568 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (collect at SimpleContext.scala:15)
14:11:31.569 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
14:11:31.570 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
14:11:31.571 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[3] at map at SimpleContext.scala:15), which has no missing parents
14:11:31.580 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 1960.0 B, free 632.0 MB)
14:11:31.589 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1282.0 B, free 632.0 MB)
14:11:31.591 [dispatcher-event-loop-1] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 10.0.2.15:44508 (size: 1282.0 B, free: 632.0 MB)
14:11:31.592 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
14:11:31.593 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at SimpleContext.scala:15)
14:11:31.595 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
14:11:31.599 [dispatcher-event-loop-0] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 6091 bytes)
14:11:31.601 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
14:11:31.616 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1475 bytes result sent to driver
14:11:31.622 [mist-akka.actor.default-dispatcher-5] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 54d0759f-9ae9-4d1e-a983-8199bf78c05e saved in MapDb
14:11:31.626 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 1 (collect at SimpleContext.scala:15) finished in 0.026 s
14:11:31.627 [task-result-getter-1] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 28 ms on localhost (1/1)
14:11:31.628 [pool-7-thread-4] INFO  o.a.spark.scheduler.DAGScheduler - Job 1 finished: collect at SimpleContext.scala:15, took 0.065541 s
14:11:31.630 [task-result-getter-1] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
HttpResponse(200 OK,List(Server: akka-http/2.4.7, Date: Thu, 18 Aug 2016 14:11:31 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{"success":true,"payload":{"result":[2,4,6,8,10,12,14,16,18,0]},"errors":[],"request":{"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}),HttpProtocol(HTTP/1.1))
14:11:31.632 [mist-akka.actor.default-dispatcher-6] WARN  a.s.Serialization(akka://mist) - Using the default Java serializer for class [scala.util.Left] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'
14:11:31.698 [mist-akka.actor.default-dispatcher-2] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 54d0759f-9ae9-4d1e-a983-8199bf78c05e removed from MapDb
14:11:31.760 [mist-akka.actor.default-dispatcher-17] INFO  i.h.mist.workerManagerTestActor - Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0))
14:11:31.767 [mist-akka.actor.default-dispatcher-5] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(None,Some(src/test/python/TestError.py),None,foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:31.809 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - foo#58aeb4e1-d8e8-4847-8bb4-8eaec47772c4 is running
14:11:31.848 [pool-7-thread-5] INFO  io.hydrosphere.mist.jobs.JobPy -  Started PythonGatewayServer on port 25333
14:11:31.871 [mist-akka.actor.default-dispatcher-2] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 58aeb4e1-d8e8-4847-8bb4-8eaec47772c4 saved in MapDb
14:11:32.365 [pool-7-thread-5] ERROR io.hydrosphere.mist.jobs.JobPy - null
14:11:32.368 [pool-7-thread-5] INFO  io.hydrosphere.mist.jobs.JobPy -  Exiting due to broken pipe from Python driver
14:11:32.369 [pool-7-thread-5] ERROR io.hydrosphere.mist.jobs.JobPy - java.lang.Exception: Error in python code: null
java.lang.Exception: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:58) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:48) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:45) [classes/:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [scala-library-2.11.8.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:53) ~[classes/:na]
	... 7 common frames omitted
HttpResponse(200 OK,List(Server: akka-http/2.4.7, Date: Thu, 18 Aug 2016 14:11:32 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{"success":false,"payload":{},"errors":["java.lang.Exception: java.lang.Exception: Error in python code: null"],"request":{"pyPath":"src/test/python/TestError.py","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}),HttpProtocol(HTTP/1.1))
14:11:32.410 [mist-akka.actor.default-dispatcher-17] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 58aeb4e1-d8e8-4847-8bb4-8eaec47772c4 removed from MapDb
Receiving Data Test, Topic : foo, Message : {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar", "className":"SimpleContext$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
Publishing Data Test, Topic : foo, Message : {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar", "className":"SimpleContext$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:32.623 [mist-akka.actor.default-dispatcher-16] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar", "className":"SimpleContext$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:32.630 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(src/test/resources/mist_examples_2.11-0.0.2.jar),None,Some(SimpleContext$),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:32.692 [mist-akka.actor.default-dispatcher-5] INFO  i.h.mist.worker.ContextNode - foo#08eee52a-e82b-47f0-9f88-aef137120b15 is running
14:11:32.741 [pool-7-thread-6] INFO  org.apache.spark.SparkContext - Starting job: collect at SimpleContext.scala:15
14:11:32.743 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 2 (collect at SimpleContext.scala:15) with 1 output partitions
14:11:32.744 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (collect at SimpleContext.scala:15)
14:11:32.744 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
14:11:32.745 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
14:11:32.746 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at SimpleContext.scala:15), which has no missing parents
14:11:32.748 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 1960.0 B, free 632.0 MB)
14:11:32.759 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1281.0 B, free 632.0 MB)
14:11:32.762 [dispatcher-event-loop-1] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 10.0.2.15:44508 (size: 1281.0 B, free: 632.0 MB)
14:11:32.764 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1012
14:11:32.764 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at SimpleContext.scala:15)
14:11:32.766 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
14:11:32.769 [dispatcher-event-loop-0] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 6092 bytes)
14:11:32.770 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
14:11:32.775 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1467 bytes result sent to driver
14:11:32.790 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 2 (collect at SimpleContext.scala:15) finished in 0.021 s
14:11:32.790 [pool-7-thread-6] INFO  o.a.spark.scheduler.DAGScheduler - Job 2 finished: collect at SimpleContext.scala:15, took 0.047833 s
Receiving Data Test, Topic : foo, Message : {"success":true,"payload":{"result":[2,4,6,8,10,12,14,16,18,0]},"errors":[],"request":{"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:32.797 [mist-akka.actor.default-dispatcher-17] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"success":true,"payload":{"result":[2,4,6,8,10,12,14,16,18,0]},"errors":[],"request":{"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:32.798 [mist-akka.actor.default-dispatcher-17] ERROR i.h.m.master.mqtt.MQTTServiceActor - DeserializationException Bad type in Json
14:11:32.799 [task-result-getter-2] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 20 ms on localhost (1/1)
14:11:32.799 [task-result-getter-2] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
14:11:32.813 [mist-akka.actor.default-dispatcher-16] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 08eee52a-e82b-47f0-9f88-aef137120b15 saved in MapDb
14:11:32.845 [mist-akka.actor.default-dispatcher-16] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 08eee52a-e82b-47f0-9f88-aef137120b15 removed from MapDb
Receiving Data Test, Topic : foo, Message : ...{"TestErrorBadJson":"TestErrorBadJson$"}
Publishing Data Test, Topic : foo, Message : ...{"TestErrorBadJson":"TestErrorBadJson$"}
14:11:37.604 [mist-akka.actor.default-dispatcher-18] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : ...{"TestErrorBadJson":"TestErrorBadJson$"}
14:11:37.606 [mist-akka.actor.default-dispatcher-18] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
Receiving Data Test, Topic : foo, Message : {"pyPath":"src/test/python/TestError.py", "parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:37.629 [mist-akka.actor.default-dispatcher-18] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"pyPath":"src/test/python/TestError.py", "parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
Publishing Data Test, Topic : foo, Message : {"pyPath":"src/test/python/TestError.py", "parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:37.639 [mist-akka.actor.default-dispatcher-5] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(None,Some(src/test/python/TestError.py),None,foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:37.650 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - foo#5ca8203c-abf8-4a7f-a4a2-c482b42fb4a1 is running
14:11:37.666 [pool-7-thread-7] INFO  io.hydrosphere.mist.jobs.JobPy -  Started PythonGatewayServer on port 25333
14:11:37.736 [mist-akka.actor.default-dispatcher-4] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 5ca8203c-abf8-4a7f-a4a2-c482b42fb4a1 saved in MapDb
14:11:38.717 [pool-7-thread-7] ERROR io.hydrosphere.mist.jobs.JobPy - null
14:11:38.720 [pool-7-thread-7] INFO  io.hydrosphere.mist.jobs.JobPy -  Exiting due to broken pipe from Python driver
14:11:38.721 [pool-7-thread-7] ERROR io.hydrosphere.mist.jobs.JobPy - java.lang.Exception: Error in python code: null
java.lang.Exception: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:58) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:48) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:45) [classes/:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [scala-library-2.11.8.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:53) ~[classes/:na]
	... 7 common frames omitted
Receiving Data Test, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.Exception: java.lang.Exception: Error in python code: null"],"request":{"pyPath":"src/test/python/TestError.py","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:38.725 [mist-akka.actor.default-dispatcher-2] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.Exception: java.lang.Exception: Error in python code: null"],"request":{"pyPath":"src/test/python/TestError.py","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:38.727 [mist-akka.actor.default-dispatcher-2] ERROR i.h.m.master.mqtt.MQTTServiceActor - DeserializationException Bad type in Json
14:11:38.751 [mist-akka.actor.default-dispatcher-5] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 5ca8203c-abf8-4a7f-a4a2-c482b42fb4a1 removed from MapDb
Receiving Data Test, Topic : foo, Message : {"jarPath":"/target/scala-2.10/bad.jar", "className":"BadclassName$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
Publishing Data Test, Topic : foo, Message : {"jarPath":"/target/scala-2.10/bad.jar", "className":"BadclassName$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:39.701 [mist-akka.actor.default-dispatcher-5] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"jarPath":"/target/scala-2.10/bad.jar", "className":"BadclassName$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:39.715 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(/target/scala-2.10/bad.jar),None,Some(BadclassName$),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:39.731 [pool-7-thread-8] ERROR io.hydrosphere.mist.jobs.Job$ - /target/scala-2.10/bad.jar (No such file or directory)
java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[na:1.8.0_101]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:93) ~[na:1.8.0_101]
	at java.io.FileReader.<init>(FileReader.java:58) ~[na:1.8.0_101]
	at io.hydrosphere.mist.jobs.Job$.apply(Job.scala:40) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:46) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:45) [classes/:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [scala-library-2.11.8.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:11:39.736 [ForkJoinPool-6-worker-3] ERROR io.hydrosphere.mist.jobs.Job$ - /target/scala-2.10/bad.jar (No such file or directory)
java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[na:1.8.0_101]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:93) ~[na:1.8.0_101]
	at java.io.FileReader.<init>(FileReader.java:58) ~[na:1.8.0_101]
	at io.hydrosphere.mist.jobs.Job$.apply(Job.scala:40) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$applyOrElse$2.applyOrElse(ContextNode.scala:56) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$applyOrElse$2.applyOrElse(ContextNode.scala:54) [classes/:na]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) [scala-library-2.11.8.jar:na]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.8.jar:na]
Receiving Data Test, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.Exception: java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)"],"request":{"jarPath":"/target/scala-2.10/bad.jar","className":"BadclassName$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:39.746 [mist-akka.actor.default-dispatcher-18] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.Exception: java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)"],"request":{"jarPath":"/target/scala-2.10/bad.jar","className":"BadclassName$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:39.750 [mist-akka.actor.default-dispatcher-18] ERROR i.h.m.master.mqtt.MQTTServiceActor - DeserializationException Bad type in Json
Receiving Data Test, Topic : foo, Message : {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar", "className":"noDoStuffMethodError","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
Publishing Data Test, Topic : foo, Message : {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar", "className":"noDoStuffMethodError","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:39.854 [mist-akka.actor.default-dispatcher-18] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar", "className":"noDoStuffMethodError","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:39.859 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(src/test/resources/mist_examples_2.11-0.0.2.jar),None,Some(noDoStuffMethodError),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
Receiving Data Test, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.NoSuchFieldException: MODULE$"],"request":{"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"noDoStuffMethodError","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:39.922 [mist-akka.actor.default-dispatcher-18] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.NoSuchFieldException: MODULE$"],"request":{"jarPath":"src/test/resources/mist_examples_2.11-0.0.2.jar","className":"noDoStuffMethodError","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:39.923 [mist-akka.actor.default-dispatcher-18] ERROR i.h.m.master.mqtt.MQTTServiceActor - DeserializationException Bad type in Json
Receiving Data Test, Topic : foo, Message : {"jarPath":"/bad.bad", "className":"SimpleContext$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
Publishing Data Test, Topic : foo, Message : {"jarPath":"/bad.bad", "className":"SimpleContext$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:39.990 [mist-akka.actor.default-dispatcher-18] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"jarPath":"/bad.bad", "className":"SimpleContext$","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]}, "external_id":"12345678","name":"foo"}
14:11:40.005 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.worker.ContextNode - [WORKER] received JobRequest: JobConfiguration(Some(/bad.bad),None,Some(SimpleContext$),foo,Map(digits -> List(1, 2, 3, 4, 5, 6, 7, 8, 9, 0)),Some(12345678))
14:11:40.018 [pool-7-thread-10] ERROR io.hydrosphere.mist.jobs.Job$ - /bad.bad (No such file or directory)
java.io.FileNotFoundException: /bad.bad (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[na:1.8.0_101]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:93) ~[na:1.8.0_101]
	at java.io.FileReader.<init>(FileReader.java:58) ~[na:1.8.0_101]
	at io.hydrosphere.mist.jobs.Job$.apply(Job.scala:40) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:46) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:45) [classes/:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) [scala-library-2.11.8.jar:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:11:40.029 [ForkJoinPool-6-worker-3] ERROR io.hydrosphere.mist.jobs.Job$ - /bad.bad (No such file or directory)
java.io.FileNotFoundException: /bad.bad (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[na:1.8.0_101]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:93) ~[na:1.8.0_101]
	at java.io.FileReader.<init>(FileReader.java:58) ~[na:1.8.0_101]
	at io.hydrosphere.mist.jobs.Job$.apply(Job.scala:40) ~[classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$applyOrElse$2.applyOrElse(ContextNode.scala:56) [classes/:na]
	at io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$applyOrElse$2.applyOrElse(ContextNode.scala:54) [classes/:na]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436) [scala-library-2.11.8.jar:na]
	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) [scala-library-2.11.8.jar:na]
	at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.11.8.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.11.8.jar:na]
Receiving Data Test, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.Exception: java.io.FileNotFoundException: /bad.bad (No such file or directory)"],"request":{"jarPath":"/bad.bad","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:40.044 [mist-akka.actor.default-dispatcher-18] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : {"success":false,"payload":{},"errors":["java.lang.Exception: java.io.FileNotFoundException: /bad.bad (No such file or directory)"],"request":{"jarPath":"/bad.bad","className":"SimpleContext$","name":"foo","parameters":{"digits":[1,2,3,4,5,6,7,8,9,0]},"external_id":"12345678"}}
14:11:40.047 [mist-akka.actor.default-dispatcher-18] ERROR i.h.m.master.mqtt.MQTTServiceActor - DeserializationException Bad type in Json
14:11:40.188 [mist-akka.actor.default-dispatcher-18] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Marked address [akka.tcp://mist@127.0.0.1:41022] as [Leaving]
14:11:40.317 [mist-akka.actor.default-dispatcher-3] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
14:11:40.330 [mist-akka.actor.default-dispatcher-4] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
14:11:40.422 [mist-akka.actor.default-dispatcher-3] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Remoting shut down.
[DEBUG] [08/18/2016 14:11:40.422] [mist-akka.actor.default-dispatcher-4] [EventStream] shutting down: StandardOutLogger started
[DEBUG] [08/18/2016 14:11:40.424] [mist-akka.actor.default-dispatcher-4] [EventStream] all default loggers stopped
[INFO] [08/18/2016 14:11:40.441] [mist-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://mist)] Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Shutting down...
[INFO] [08/18/2016 14:11:40.446] [mist-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://mist)] Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Successfully shut down
14:11:40.465 [mist-akka.actor.default-dispatcher-6] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
14:11:40.468 [mist-akka.actor.default-dispatcher-6] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
[DEBUG] [08/18/2016 14:11:40.468] [mist-akka.actor.default-dispatcher-2] [EventStream] shutting down: StandardOutLogger started
14:11:40.470 [mist-akka.actor.default-dispatcher-6] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Remoting shut down.
[DEBUG] [08/18/2016 14:11:40.471] [mist-akka.actor.default-dispatcher-2] [EventStream] all default loggers stopped
[DEBUG] [08/18/2016 14:11:40.468] [mist-akka.actor.default-dispatcher-2] [EventStream] shutting down: StandardOutLogger started
[INFO] [08/18/2016 14:11:40.476] [mist-akka.actor.default-dispatcher-5] [akka.cluster.Cluster(akka://mist)] Cluster Node [akka.tcp://mist@127.0.0.1:41022] - Shutting down...
[INFO] [08/18/2016 14:11:40.477] [mist-akka.actor.default-dispatcher-5] [akka.cluster.Cluster(akka://mist)] Cluster Node [akka.tcp://mist@127.0.0.1:41022] - Successfully shut down
[0m[[0minfo[0m] [0m[32mworkerManagerTestActor:[0m[0m
[0m[[0minfo[0m] [0m[32mContextNode[0m[0m
[0m[[0minfo[0m] [0m[32m- must started[0m[0m
[0m[[0minfo[0m] [0m[32m- must message[0m[0m
[0m[[0minfo[0m] [0m[32m- must http bad request[0m[0m
[0m[[0minfo[0m] [0m[32m- must http bad patch[0m[0m
[0m[[0minfo[0m] [0m[32m- must HTTP bad JSON[0m[0m
[0m[[0minfo[0m] [0m[32m- must http jar no do stuff[0m[0m
[0m[[0minfo[0m] [0m[32m- must HTTP Spark Context jar[0m[0m
[0m[[0minfo[0m] [0m[32m- must HTTP error in python[0m[0m
[0m[[0minfo[0m] [0m[33m- must HTTP Pyspark Context !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:332)[0m[0m
[0m[[0minfo[0m] [0m[33m- must HTTP SparkSQL !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:359)[0m[0m
[0m[[0minfo[0m] [0m[33m- must HTTP Python SparkSQL !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:386)[0m[0m
[0m[[0minfo[0m] [0m[33m- must HTTP Spark HIVE !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:413)[0m[0m
[0m[[0minfo[0m] [0m[33m- must HTTP Python Spark HIVE !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:440)[0m[0m
[0m[[0minfo[0m] [0m[32m- must mqtt jar[0m[0m
[0m[[0minfo[0m] [0m[33m- must mqtt spark sql !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:480)[0m[0m
[0m[[0minfo[0m] [0m[33m- must mqtt python spark !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:505)[0m[0m
[0m[[0minfo[0m] [0m[32m- must MQTT bad JSON[0m[0m
[0m[[0minfo[0m] [0m[33m- must MQTT Spark HIVE !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:526)[0m[0m
[0m[[0minfo[0m] [0m[33m- must MQTT Python Spark HIVE !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (WorkerTest.scala:537)[0m[0m
[0m[[0minfo[0m] [0m[32m- must MQTT error in Python[0m[0m
[0m[[0minfo[0m] [0m[32m- must MQTT bad path[0m[0m
[0m[[0minfo[0m] [0m[32m- must MQTT noDoStuff in jar[0m[0m
[0m[[0minfo[0m] [0m[32m- must MQTT bad extension in path[0m[0m
[0m[[0minfo[0m] [0m[32m- must stopped[0m[0m
[DEBUG] [08/18/2016 14:11:45.831] [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] [EventStream] StandardOutLogger started
14:11:45.843 [mist-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[DEBUG] [08/18/2016 14:11:45.845] [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] [EventStream(akka://mist)] logger log1-Slf4jLogger started
[DEBUG] [08/18/2016 14:11:45.846] [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] [EventStream(akka://mist)] Default Loggers started
14:11:45.858 [mist-akka.actor.default-dispatcher-7] INFO  akka.remote.Remoting - Starting remoting
14:11:45.882 [mist-akka.actor.default-dispatcher-7] INFO  akka.remote.Remoting - Remoting started; listening on addresses :[akka.tcp://mist@127.0.0.1:2551]
14:11:45.886 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Starting up...
14:11:45.890 [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] INFO  io.hydrosphere.mist.Master$ - 0
14:11:45.894 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Registered cluster JMX MBean [akka:type=Cluster]
14:11:45.895 [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] INFO  io.hydrosphere.mist.Master$ - 2551
14:11:45.902 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Started up successfully
14:11:45.905 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Metrics will be retreived from MBeans, and may be incorrect on some platforms. To increase metric accuracy add the 'sigar.jar' to the classpath and the appropriate platform-specific native libary to 'java.library.path'. Reason: java.lang.ClassNotFoundException: org.hyperic.sigar.Sigar
14:11:45.906 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Metrics collection has started successfully
14:11:45.906 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Node [akka.tcp://mist@127.0.0.1:2551] is JOINING, roles []
14:11:45.907 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Leader is moving node [akka.tcp://mist@127.0.0.1:2551] to [Up]
14:11:45.934 [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] INFO  io.hydrosphere.mist.Master$ - Creating contexts which are specified in config
14:11:45.968 [mist-akka.actor.default-dispatcher-5] INFO  i.h.mist.master.mqtt.MqttPubSub - connecting to tcp://192.168.10.33:1883..
14:11:45.978 [mist-akka.actor.default-dispatcher-18] INFO  i.h.mist.master.mqtt.MqttPubSub - connecting to tcp://192.168.10.33:1883..
14:11:46.030 [MQTT Call: paho781437866446257] INFO  i.h.m.m.m.MqttPubSub$SubscribeListener$ - subscribed to [foo]
14:11:46.034 [mist-akka.actor.default-dispatcher-3] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 0 get from MapDb
14:11:46.034 [mist-akka.actor.default-dispatcher-3] INFO  i.h.mist.master.JobRecovery - 0 loaded from MapDb 
14:11:46.062 [mist-akka.actor.default-dispatcher-3] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
[DEBUG] [08/18/2016 14:12:06.024] [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] [EventStream] StandardOutLogger started
14:12:06.041 [mist-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
[DEBUG] [08/18/2016 14:12:06.042] [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] [EventStream(akka://mist)] logger log1-Slf4jLogger started
[DEBUG] [08/18/2016 14:12:06.043] [pool-4-thread-2-ScalaTest-running-MasterWorkerAppsTest] [EventStream(akka://mist)] Default Loggers started
14:12:06.049 [mist-akka.actor.default-dispatcher-4] INFO  akka.remote.Remoting - Starting remoting
14:12:06.066 [mist-akka.actor.default-dispatcher-7] INFO  akka.remote.Remoting - Remoting started; listening on addresses :[akka.tcp://mist@127.0.0.1:39001]
14:12:06.069 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:39001] - Starting up...
14:12:06.070 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:39001] - Started up successfully
14:12:06.073 [mist-akka.actor.default-dispatcher-5] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:39001] - Metrics will be retreived from MBeans, and may be incorrect on some platforms. To increase metric accuracy add the 'sigar.jar' to the classpath and the appropriate platform-specific native libary to 'java.library.path'. Reason: java.lang.ClassNotFoundException: org.hyperic.sigar.Sigar
14:12:06.073 [mist-akka.actor.default-dispatcher-5] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:39001] - Metrics collection has started successfully
14:12:06.098 [mist-akka.actor.default-dispatcher-6] INFO  i.h.mist.master.WorkerManager - Worker `foo` did start on akka.tcp://mist@127.0.0.1:39001
14:12:06.099 [mist-akka.actor.default-dispatcher-18] WARN  a.s.Serialization(akka://mist) - Using the default Java serializer for class [io.hydrosphere.mist.Messages$WorkerDidStart] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'
14:12:06.101 [mist-akka.actor.default-dispatcher-6] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Node [akka.tcp://mist@127.0.0.1:39001] is JOINING, roles []
14:12:06.103 [mist-akka.actor.default-dispatcher-4] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:39001] - Welcome from [akka.tcp://mist@127.0.0.1:2551]
14:12:06.900 [mist-akka.actor.default-dispatcher-18] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Leader is moving node [akka.tcp://mist@127.0.0.1:39001] to [Up]
[0m[[0minfo[0m] [0m[32mMasterWorkerAppsTest:[0m[0m
[0m[[0minfo[0m] [0m[32mMust started[0m[0m
[0m[[0minfo[0m] [0m[32m- must Master App[0m[0m
[0m[[0minfo[0m] [0m[32m- must Worker App[0m[0m
14:12:26.157 [MqttTestActorSystem-akka.actor.default-dispatcher-4] INFO  i.h.mist.master.mqtt.MqttPubSub - connecting to tcp://192.168.10.33:1883..
14:12:26.179 [MQTT Call: paho781478055707589] INFO  i.h.m.m.m.MqttPubSub$SubscribeListener$ - subscribed to [foo]
Receiving Data Test, Topic : foo, Message : testmsg
14:12:26.239 [mist-akka.actor.default-dispatcher-4] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.243 [mist-akka.actor.default-dispatcher-4] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.243 [mist-akka.actor.default-dispatcher-4] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.244 [mist-akka.actor.default-dispatcher-4] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.246 [mist-akka.actor.default-dispatcher-4] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.250 [mist-akka.actor.default-dispatcher-4] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.256 [mist-akka.actor.default-dispatcher-4] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.258 [mist-akka.actor.default-dispatcher-4] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.261 [mist-akka.actor.default-dispatcher-4] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.263 [mist-akka.actor.default-dispatcher-4] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.265 [mist-akka.actor.default-dispatcher-6] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.265 [mist-akka.actor.default-dispatcher-6] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.266 [mist-akka.actor.default-dispatcher-6] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.267 [mist-akka.actor.default-dispatcher-6] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.268 [mist-akka.actor.default-dispatcher-6] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.270 [mist-akka.actor.default-dispatcher-6] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.272 [mist-akka.actor.default-dispatcher-6] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.274 [mist-akka.actor.default-dispatcher-6] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
14:12:26.275 [mist-akka.actor.default-dispatcher-6] INFO  i.h.m.master.mqtt.MQTTServiceActor - Receiving Data, Topic : foo, Message : testmsg
14:12:26.279 [mist-akka.actor.default-dispatcher-6] ERROR i.h.m.master.mqtt.MQTTServiceActor - BadJson
MqttException (0) - java.util.NoSuchElementException: next on empty iterator
1: Pub 10 Rec 10 ~ 100.0%
[0m[[0minfo[0m] [0m[32mMqttTest:[0m[0m
[0m[[0minfo[0m] [0m[32mMqttPubSub[0m[0m
[0m[[0minfo[0m] [0m[32m- must Mqtt ok[0m[0m
14:12:32.399 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
14:12:32.416 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 1 saved in MapDb
14:12:32.429 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 2 saved in MapDb
14:12:32.452 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 3 saved in MapDb
14:12:32.558 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
14:12:32.596 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
14:12:32.615 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 1 saved in MapDb
14:12:32.666 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
14:12:32.679 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 1 saved in MapDb
14:12:32.693 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 1 removed from MapDb
14:12:32.735 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
14:12:32.751 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 1 saved in MapDb
14:12:32.808 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
14:12:32.830 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 1 saved in MapDb
14:12:32.844 [pool-4-thread-2-ScalaTest-running-JobRepositoryTest] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - 1 get from MapDb
14:12:32.904 [pool-4-thread-2] INFO  i.h.m.j.InMapDbJobConfigurationRepository$ - MpDb cleaned
[0m[[0minfo[0m] [0m[32mJobRepositoryTest:[0m[0m
[0m[[0minfo[0m] [0m[32m- size InMapDbJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- Clear InMapDbJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- Add in InMapDbJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- Remove from InMapDbJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- get from InMapDbJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- getAll from InMapDbJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- size InMemoryJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- Clear InMemoryJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- Add in InMemoryJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- Remove from InMemoryJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- get from InMemoryJobConfigurationRepository[0m[0m
[0m[[0minfo[0m] [0m[32m- getAll from InMemoryJobConfigurationRepository[0m[0m
14:12:32.953 [pool-4-thread-2] WARN  org.apache.spark.SparkContext - Multiple running SparkContexts detected in the same JVM!
org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.<init>(SparkContext.scala:80)
io.hydrosphere.mist.contexts.ContextBuilder$.namedSparkContext(ContextBuilder.scala:27)
io.hydrosphere.mist.worker.ContextNode.contextWrapper$lzycompute(ContextNode.scala:27)
io.hydrosphere.mist.worker.ContextNode.contextWrapper(ContextNode.scala:27)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:46)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:45)
scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
java.lang.Thread.run(Thread.java:745)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2221) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2217) [spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) [scala-library-2.11.8.jar:0.13.7]
	at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2217) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2290) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:89) [spark-core_2.11-2.0.0.jar:2.0.0]
	at io.hydrosphere.mist.contexts.ContextBuilder$.namedSparkContext(ContextBuilder.scala:27) [classes/:na]
	at io.hydrosphere.mist.JobTests.<init>(JobTests.scala:125) [test-classes/:na]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) [na:1.8.0_101]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [na:1.8.0_101]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [na:1.8.0_101]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [na:1.8.0_101]
	at java.lang.Class.newInstance(Class.java:442) [na:1.8.0_101]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:468) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:12:32.957 [pool-4-thread-2] INFO  org.apache.spark.SparkContext - Running Spark version 2.0.0
14:12:32.962 [pool-4-thread-2] INFO  org.apache.spark.SecurityManager - Changing view acls to: vagrant
14:12:32.962 [pool-4-thread-2] INFO  org.apache.spark.SecurityManager - Changing modify acls to: vagrant
14:12:32.962 [pool-4-thread-2] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
14:12:32.963 [pool-4-thread-2] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
14:12:32.963 [pool-4-thread-2] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(vagrant); groups with view permissions: Set(); users  with modify permissions: Set(vagrant); groups with modify permissions: Set()
14:12:32.969 [pool-4-thread-2] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 36841.
14:12:32.975 [pool-4-thread-2] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
14:12:32.979 [pool-4-thread-2] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
14:12:32.981 [pool-4-thread-2] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at /tmp/blockmgr-f0be9098-e807-47c7-b86f-1c5ca7308628
14:12:32.982 [pool-4-thread-2] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 632.0 MB
14:12:32.988 [pool-4-thread-2] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
14:12:33.029 [pool-4-thread-2] INFO  o.spark_project.jetty.server.Server - jetty-9.2.z-SNAPSHOT
14:12:33.032 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e5a10ff{/jobs,null,AVAILABLE}
14:12:33.034 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@250e0bae{/jobs/json,null,AVAILABLE}
14:12:33.034 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25e7069a{/jobs/job,null,AVAILABLE}
14:12:33.035 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b242747{/jobs/job/json,null,AVAILABLE}
14:12:33.035 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45e71934{/stages,null,AVAILABLE}
14:12:33.037 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1061395d{/stages/json,null,AVAILABLE}
14:12:33.038 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d2f2e1d{/stages/stage,null,AVAILABLE}
14:12:33.041 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@774b2ee0{/stages/stage/json,null,AVAILABLE}
14:12:33.042 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a18b784{/stages/pool,null,AVAILABLE}
14:12:33.043 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@132acc4b{/stages/pool/json,null,AVAILABLE}
14:12:33.043 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@670147f5{/storage,null,AVAILABLE}
14:12:33.044 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@491349db{/storage/json,null,AVAILABLE}
14:12:33.044 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67863ea{/storage/rdd,null,AVAILABLE}
14:12:33.045 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43f5bffa{/storage/rdd/json,null,AVAILABLE}
14:12:33.045 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e26769e{/environment,null,AVAILABLE}
14:12:33.047 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d28bf10{/environment/json,null,AVAILABLE}
14:12:33.047 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@388ae46c{/executors,null,AVAILABLE}
14:12:33.048 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56ee96e8{/executors/json,null,AVAILABLE}
14:12:33.048 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b61982d{/executors/threadDump,null,AVAILABLE}
14:12:33.049 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73896e6e{/executors/threadDump/json,null,AVAILABLE}
14:12:33.063 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ccf5df7{/static,null,AVAILABLE}
14:12:33.064 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e34d150{/,null,AVAILABLE}
14:12:33.067 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7953964b{/api,null,AVAILABLE}
14:12:33.068 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41117d11{/stages/stage/kill,null,AVAILABLE}
14:12:33.071 [pool-4-thread-2] WARN  o.s.j.u.component.AbstractLifeCycle - FAILED ServerConnector@1a49855f{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method) ~[na:1.8.0_101]
	at sun.nio.ch.Net.bind(Net.java:433) ~[na:1.8.0_101]
	at sun.nio.ch.Net.bind(Net.java:425) ~[na:1.8.0_101]
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[na:1.8.0_101]
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[na:1.8.0_101]
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.server.Server.doStart(Server.java:366) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:298) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:308) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:308) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2071) [spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160) [scala-library-2.11.8.jar:0.13.7]
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2062) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:308) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:139) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:451) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:451) [spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) [scala-library-2.11.8.jar:0.13.7]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:451) [spark-core_2.11-2.0.0.jar:2.0.0]
	at io.hydrosphere.mist.contexts.ContextBuilder$.namedSparkContext(ContextBuilder.scala:27) [classes/:na]
	at io.hydrosphere.mist.JobTests.<init>(JobTests.scala:125) [test-classes/:na]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) [na:1.8.0_101]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [na:1.8.0_101]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [na:1.8.0_101]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [na:1.8.0_101]
	at java.lang.Class.newInstance(Class.java:442) [na:1.8.0_101]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:468) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:12:33.078 [pool-4-thread-2] WARN  o.s.j.u.component.AbstractLifeCycle - FAILED org.spark_project.jetty.server.Server@2bfedf0c: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method) ~[na:1.8.0_101]
	at sun.nio.ch.Net.bind(Net.java:433) ~[na:1.8.0_101]
	at sun.nio.ch.Net.bind(Net.java:425) ~[na:1.8.0_101]
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[na:1.8.0_101]
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[na:1.8.0_101]
	at org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.server.Server.doStart(Server.java:366) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) ~[spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:298) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:308) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:308) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2071) [spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160) [scala-library-2.11.8.jar:0.13.7]
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2062) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:308) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:139) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:451) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:451) [spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) [scala-library-2.11.8.jar:0.13.7]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:451) [spark-core_2.11-2.0.0.jar:2.0.0]
	at io.hydrosphere.mist.contexts.ContextBuilder$.namedSparkContext(ContextBuilder.scala:27) [classes/:na]
	at io.hydrosphere.mist.JobTests.<init>(JobTests.scala:125) [test-classes/:na]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) [na:1.8.0_101]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [na:1.8.0_101]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [na:1.8.0_101]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [na:1.8.0_101]
	at java.lang.Class.newInstance(Class.java:442) [na:1.8.0_101]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:468) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:12:33.098 [pool-4-thread-2] INFO  o.s.jetty.server.ServerConnector - Stopped ServerConnector@1a49855f{HTTP/1.1}{0.0.0.0:4040}
14:12:33.103 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@41117d11{/stages/stage/kill,null,UNAVAILABLE}
14:12:33.105 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7953964b{/api,null,UNAVAILABLE}
14:12:33.105 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3e34d150{/,null,UNAVAILABLE}
14:12:33.106 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1ccf5df7{/static,null,UNAVAILABLE}
14:12:33.108 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@73896e6e{/executors/threadDump/json,null,UNAVAILABLE}
14:12:33.116 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4b61982d{/executors/threadDump,null,UNAVAILABLE}
14:12:33.117 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@56ee96e8{/executors/json,null,UNAVAILABLE}
14:12:33.117 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@388ae46c{/executors,null,UNAVAILABLE}
14:12:33.117 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7d28bf10{/environment/json,null,UNAVAILABLE}
14:12:33.118 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2e26769e{/environment,null,UNAVAILABLE}
14:12:33.118 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@43f5bffa{/storage/rdd/json,null,UNAVAILABLE}
14:12:33.119 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@67863ea{/storage/rdd,null,UNAVAILABLE}
14:12:33.120 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@491349db{/storage/json,null,UNAVAILABLE}
14:12:33.120 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@670147f5{/storage,null,UNAVAILABLE}
14:12:33.120 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@132acc4b{/stages/pool/json,null,UNAVAILABLE}
14:12:33.121 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@a18b784{/stages/pool,null,UNAVAILABLE}
14:12:33.121 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@774b2ee0{/stages/stage/json,null,UNAVAILABLE}
14:12:33.129 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d2f2e1d{/stages/stage,null,UNAVAILABLE}
14:12:33.130 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1061395d{/stages/json,null,UNAVAILABLE}
14:12:33.132 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@45e71934{/stages,null,UNAVAILABLE}
14:12:33.134 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5b242747{/jobs/job/json,null,UNAVAILABLE}
14:12:33.138 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@25e7069a{/jobs/job,null,UNAVAILABLE}
14:12:33.139 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@250e0bae{/jobs/json,null,UNAVAILABLE}
14:12:33.141 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4e5a10ff{/jobs,null,UNAVAILABLE}
14:12:33.147 [pool-4-thread-2] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
14:12:33.149 [pool-4-thread-2] INFO  o.spark_project.jetty.server.Server - jetty-9.2.z-SNAPSHOT
14:12:33.155 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e5a10ff{/jobs,null,AVAILABLE}
14:12:33.156 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@250e0bae{/jobs/json,null,AVAILABLE}
14:12:33.156 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25e7069a{/jobs/job,null,AVAILABLE}
14:12:33.160 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b242747{/jobs/job/json,null,AVAILABLE}
14:12:33.161 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45e71934{/stages,null,AVAILABLE}
14:12:33.162 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1061395d{/stages/json,null,AVAILABLE}
14:12:33.162 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d2f2e1d{/stages/stage,null,AVAILABLE}
14:12:33.163 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@774b2ee0{/stages/stage/json,null,AVAILABLE}
14:12:33.163 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@a18b784{/stages/pool,null,AVAILABLE}
14:12:33.164 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@132acc4b{/stages/pool/json,null,AVAILABLE}
14:12:33.166 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@670147f5{/storage,null,AVAILABLE}
14:12:33.166 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@491349db{/storage/json,null,AVAILABLE}
14:12:33.167 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@67863ea{/storage/rdd,null,AVAILABLE}
14:12:33.167 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@43f5bffa{/storage/rdd/json,null,AVAILABLE}
14:12:33.168 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e26769e{/environment,null,AVAILABLE}
14:12:33.168 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d28bf10{/environment/json,null,AVAILABLE}
14:12:33.170 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@388ae46c{/executors,null,AVAILABLE}
14:12:33.170 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56ee96e8{/executors/json,null,AVAILABLE}
14:12:33.171 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4b61982d{/executors/threadDump,null,AVAILABLE}
14:12:33.171 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73896e6e{/executors/threadDump/json,null,AVAILABLE}
14:12:33.172 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ccf5df7{/static,null,AVAILABLE}
14:12:33.172 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e34d150{/,null,AVAILABLE}
14:12:33.173 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7953964b{/api,null,AVAILABLE}
14:12:33.174 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41117d11{/stages/stage/kill,null,AVAILABLE}
14:12:33.177 [pool-4-thread-2] INFO  o.s.jetty.server.ServerConnector - Started ServerConnector@3fa7b8dc{HTTP/1.1}{0.0.0.0:4041}
14:12:33.178 [pool-4-thread-2] INFO  o.spark_project.jetty.server.Server - Started @252111ms
14:12:33.178 [pool-4-thread-2] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
14:12:33.178 [pool-4-thread-2] INFO  org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://localhost:4041
14:12:33.311 [pool-4-thread-2] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
14:12:33.329 [pool-4-thread-2] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 47525.
14:12:33.330 [pool-4-thread-2] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on localhost:47525
14:12:33.332 [pool-4-thread-2] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, localhost, 47525)
14:12:33.333 [dispatcher-event-loop-0] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager localhost:47525 with 632.0 MB RAM, BlockManagerId(driver, localhost, 47525)
14:12:33.334 [pool-4-thread-2] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, localhost, 47525)
14:12:33.335 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6109798f{/metrics/json,null,AVAILABLE}
14:12:33.338 [pool-4-thread-2] WARN  org.apache.spark.SparkContext - Multiple running SparkContexts detected in the same JVM!
org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.SparkContext.<init>(SparkContext.scala:80)
io.hydrosphere.mist.contexts.ContextBuilder$.namedSparkContext(ContextBuilder.scala:27)
io.hydrosphere.mist.worker.ContextNode.contextWrapper$lzycompute(ContextNode.scala:27)
io.hydrosphere.mist.worker.ContextNode.contextWrapper(ContextNode.scala:27)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.job$lzycompute$1(ContextNode.scala:43)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1.io$hydrosphere$mist$worker$ContextNode$$anonfun$$job$1(ContextNode.scala:43)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:46)
io.hydrosphere.mist.worker.ContextNode$$anonfun$receive$1$$anonfun$1.apply(ContextNode.scala:45)
scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
java.lang.Thread.run(Thread.java:745)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2221) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2217) [spark-core_2.11-2.0.0.jar:2.0.0]
	at scala.Option.foreach(Option.scala:257) [scala-library-2.11.8.jar:0.13.7]
	at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2217) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext$.setActiveContext(SparkContext.scala:2303) [spark-core_2.11-2.0.0.jar:2.0.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:2173) [spark-core_2.11-2.0.0.jar:2.0.0]
	at io.hydrosphere.mist.contexts.ContextBuilder$.namedSparkContext(ContextBuilder.scala:27) [classes/:na]
	at io.hydrosphere.mist.JobTests.<init>(JobTests.scala:125) [test-classes/:na]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) [na:1.8.0_101]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [na:1.8.0_101]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [na:1.8.0_101]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [na:1.8.0_101]
	at java.lang.Class.newInstance(Class.java:442) [na:1.8.0_101]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:468) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:12:38.431 [pool-4-thread-2-ScalaTest-running-JobTests] ERROR io.hydrosphere.mist.jobs.Job$ - /target/scala-2.10/bad.jar (No such file or directory)
java.io.FileNotFoundException: /target/scala-2.10/bad.jar (No such file or directory)
	at java.io.FileInputStream.open0(Native Method) ~[na:1.8.0_101]
	at java.io.FileInputStream.open(FileInputStream.java:195) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.8.0_101]
	at java.io.FileInputStream.<init>(FileInputStream.java:93) ~[na:1.8.0_101]
	at java.io.FileReader.<init>(FileReader.java:58) ~[na:1.8.0_101]
	at io.hydrosphere.mist.jobs.Job$.apply(Job.scala:40) ~[classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$13$$anonfun$apply$mcV$sp$1.apply$mcV$sp(JobTests.scala:145) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$13$$anonfun$apply$mcV$sp$1.apply(JobTests.scala:142) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$13$$anonfun$apply$mcV$sp$1.apply(JobTests.scala:142) [test-classes/:na]
	at org.scalatest.Assertions$class.intercept(Assertions.scala:997) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.intercept(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests$$anonfun$13.apply$mcV$sp(JobTests.scala:142) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$13.apply(JobTests.scala:142) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$13.apply(JobTests.scala:142) [test-classes/:na]
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:20) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at scala.collection.immutable.List.foreach(List.scala:381) [scala-library-2.11.8.jar:0.13.7]
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.run(Suite.scala:1424) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.org$scalatest$BeforeAndAfterAll$$super$run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:12:38.440 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  org.apache.spark.SparkContext - Added JAR src/test/resources/mist_examples_2.11-0.0.2.jar at spark://localhost:36841/jars/mist_examples_2.11-0.0.2.jar with timestamp 1471529558439
14:12:38.699 [pool-4-thread-2-ScalaTest-running-JobTests] WARN  org.apache.spark.SparkContext - Use an existing SparkContext, some configuration may not take effect.
14:12:38.767 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@61c2124f{/SQL,null,AVAILABLE}
14:12:38.770 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@33f3ffe0{/SQL/json,null,AVAILABLE}
14:12:38.788 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49f1dcdf{/SQL/execution,null,AVAILABLE}
14:12:38.791 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dfd5273{/SQL/execution/json,null,AVAILABLE}
14:12:38.813 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3c06a31b{/static/sql,null,AVAILABLE}
14:12:38.841 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.spark.sql.hive.HiveSharedState - Warehouse path is 'file:/vagrant/spark-warehouse'.
14:12:41.313 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 133.0 KB, free 631.9 MB)
14:12:41.363 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.8 KB, free 631.9 MB)
14:12:41.364 [dispatcher-event-loop-0] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:47525 (size: 14.8 KB, free: 632.0 MB)
14:12:41.369 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  org.apache.spark.SparkContext - Created broadcast 0 from json at SimpleHiveContext_SparkSession.scala:15
14:12:41.611 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.hadoop.mapred.FileInputFormat - Total input paths to process : 1
14:12:41.647 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  org.apache.spark.SparkContext - Starting job: json at SimpleHiveContext_SparkSession.scala:15
14:12:41.648 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 0 (json at SimpleHiveContext_SparkSession.scala:15) with 1 output partitions
14:12:41.648 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (json at SimpleHiveContext_SparkSession.scala:15)
14:12:41.649 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List()
14:12:41.649 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List()
14:12:41.650 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SimpleHiveContext_SparkSession.scala:15), which has no missing parents
14:12:41.672 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 631.9 MB)
14:12:41.679 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 631.9 MB)
14:12:41.682 [dispatcher-event-loop-1] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:47525 (size: 2.5 KB, free: 632.0 MB)
14:12:41.684 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1012
14:12:41.684 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SimpleHiveContext_SparkSession.scala:15)
14:12:41.685 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
14:12:41.708 [dispatcher-event-loop-0] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5461 bytes)
14:12:41.715 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
14:12:41.720 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Fetching spark://localhost:36841/jars/mist_examples_2.11-0.0.2.jar with timestamp 1471529558439
14:12:41.800 [Executor task launch worker-0] INFO  o.a.s.n.c.TransportClientFactory - Successfully created connection to localhost/127.0.0.1:36841 after 8 ms (0 ms spent in bootstraps)
14:12:41.801 [Executor task launch worker-0] INFO  org.apache.spark.util.Utils - Fetching spark://localhost:36841/jars/mist_examples_2.11-0.0.2.jar to /tmp/spark-ecfb90b3-5ac5-44fc-a32a-9a213ce9448d/userFiles-ad55d42e-cc4f-44b3-abbb-2995d2523a8a/fetchFileTemp8230367734776725694.tmp
14:12:41.847 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Adding file:/tmp/spark-ecfb90b3-5ac5-44fc-a32a-9a213ce9448d/userFiles-ad55d42e-cc4f-44b3-abbb-2995d2523a8a/mist_examples_2.11-0.0.2.jar to class loader
14:12:41.881 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/vagrant/src/test/resources/SimpleSQLContextData.json:0+68
14:12:41.901 [Executor task launch worker-0] INFO  o.a.h.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
14:12:41.902 [Executor task launch worker-0] INFO  o.a.h.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
14:12:41.902 [Executor task launch worker-0] INFO  o.a.h.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
14:12:41.902 [Executor task launch worker-0] INFO  o.a.h.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
14:12:41.902 [Executor task launch worker-0] INFO  o.a.h.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
14:12:42.407 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1871 bytes result sent to driver
14:12:42.430 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 0 (json at SimpleHiveContext_SparkSession.scala:15) finished in 0.733 s
14:12:42.431 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.spark.scheduler.DAGScheduler - Job 0 finished: json at SimpleHiveContext_SparkSession.scala:15, took 0.783919 s
14:12:42.433 [task-result-getter-0] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 743 ms on localhost (1/1)
14:12:42.433 [task-result-getter-0] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
14:12:43.069 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
14:12:43.439 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
14:12:43.442 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
14:12:43.445 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
14:12:43.447 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
14:12:43.450 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
14:12:43.452 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
14:12:43.452 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
14:12:43.453 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.conf.Configuration.deprecation - mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
14:12:44.125 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
14:12:44.178 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.ObjectStore - ObjectStore, initialize called
14:12:49.088 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
14:12:54.207 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.h.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is DERBY
14:12:54.214 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.ObjectStore - Initialized ObjectStore
14:12:54.975 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.HiveMetaStore - Added admin role in metastore
14:12:54.978 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.HiveMetaStore - Added public role in metastore
14:12:55.144 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
14:12:55.403 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.HiveMetaStore - 0: get_all_databases
14:12:55.422 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.h.m.HiveMetaStore.audit - ugi=vagrant	ip=unknown-ip-addr	cmd=get_all_databases	
14:12:55.488 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
14:12:55.489 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.h.m.HiveMetaStore.audit - ugi=vagrant	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
14:12:55.672 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created local directory: /tmp/b20ce519-e34b-47a0-8924-845893a216f5_resources
14:12:55.697 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/vagrant/b20ce519-e34b-47a0-8924-845893a216f5
14:12:55.708 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created local directory: /tmp/vagrant/b20ce519-e34b-47a0-8924-845893a216f5
14:12:55.732 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/vagrant/b20ce519-e34b-47a0-8924-845893a216f5/_tmp_space.db
14:12:55.783 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/vagrant/spark-warehouse
14:12:55.946 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created local directory: /tmp/18c4ff04-0289-4ae5-872b-b0a17ca9a0f9_resources
14:12:55.958 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/vagrant/18c4ff04-0289-4ae5-872b-b0a17ca9a0f9
14:12:55.970 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created local directory: /tmp/vagrant/18c4ff04-0289-4ae5-872b-b0a17ca9a0f9
14:12:55.987 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/vagrant/18c4ff04-0289-4ae5-872b-b0a17ca9a0f9/_tmp_space.db
14:12:55.989 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/vagrant/spark-warehouse
14:12:56.651 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.hive.metastore.HiveMetaStore - 0: create_database: Database(name:default, description:default database, locationUri:file:/vagrant/spark-warehouse, parameters:{})
14:12:56.652 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.h.h.m.HiveMetaStore.audit - ugi=vagrant	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/vagrant/spark-warehouse, parameters:{})	
14:12:56.663 [pool-4-thread-2-ScalaTest-running-JobTests] ERROR o.a.h.h.metastore.RetryingHMSHandler - AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy22.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy23.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:287)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)
	at SimpleHiveContext_SparkSession$.doStuff(SimpleHiveContext_SparkSession.scala:15)
	at io.hydrosphere.mist.jobs.JobJarRun$class.run(JobJarRun_SparkSession.scala:39)
	at io.hydrosphere.mist.jobs.JobJar.run(JobJar.scala:13)
	at io.hydrosphere.mist.JobTests$$anonfun$22.apply$mcV$sp(JobTests.scala:252)
	at io.hydrosphere.mist.JobTests$$anonfun$22.apply(JobTests.scala:243)
	at io.hydrosphere.mist.JobTests$$anonfun$22.apply(JobTests.scala:243)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at io.hydrosphere.mist.JobTests.org$scalatest$BeforeAndAfterAll$$super$run(JobTests.scala:119)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at io.hydrosphere.mist.JobTests.run(JobTests.scala:119)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
	at sbt.TestRunner.runTest$1(TestFramework.scala:76)
	at sbt.TestRunner.run(TestFramework.scala:85)
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202)
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202)
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185)
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202)
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202)
	at sbt.TestFunction.apply(TestFramework.scala:207)
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239)
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245)
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245)
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44)
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44)
	at sbt.std.Transform$$anon$4.work(System.scala:63)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)
	at sbt.Execute.work(Execute.scala:235)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

root
 |-- _corrupt_record: string (nullable = true)
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

14:12:57.649 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.sql.execution.SparkSqlParser - Parsing command: people
14:12:58.338 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.sql.execution.SparkSqlParser - Parsing command: SELECT AVG(age) AS avg_age FROM people
14:12:59.168 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruning directories with: 
14:12:59.174 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.e.d.FileSourceStrategy - Post-Scan Filters: 
14:12:59.192 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.e.d.FileSourceStrategy - Pruned Data Schema: struct<age: bigint>
14:12:59.195 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.e.d.FileSourceStrategy - Pushed Filters: 
14:12:59.239 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 132.2 KB, free 631.7 MB)
14:12:59.271 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.6 KB, free 631.7 MB)
14:12:59.275 [dispatcher-event-loop-1] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:47525 (size: 14.6 KB, free: 632.0 MB)
14:12:59.277 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  org.apache.spark.SparkContext - Created broadcast 2 from collect at SimpleHiveContext_SparkSession.scala:19
14:12:59.294 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.e.d.FileSourceStrategy - Planning scan with bin packing, max size: 4194372 bytes, open cost is considered as scanning 4194304 bytes.
14:13:00.280 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 545.895157 ms
14:13:00.371 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 59.327823 ms
14:13:00.551 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  org.apache.spark.SparkContext - Starting job: collect at SimpleHiveContext_SparkSession.scala:19
14:13:00.557 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Registering RDD 6 (collect at SimpleHiveContext_SparkSession.scala:19)
14:13:00.563 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Got job 1 (collect at SimpleHiveContext_SparkSession.scala:19) with 1 output partitions
14:13:00.566 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (collect at SimpleHiveContext_SparkSession.scala:19)
14:13:00.568 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
14:13:00.570 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
14:13:00.582 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at collect at SimpleHiveContext_SparkSession.scala:19), which has no missing parents
14:13:00.632 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 11.4 KB, free 631.7 MB)
14:13:00.640 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.9 KB, free 631.7 MB)
14:13:00.641 [dispatcher-event-loop-0] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:47525 (size: 5.9 KB, free: 632.0 MB)
14:13:00.645 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
14:13:00.675 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at collect at SimpleHiveContext_SparkSession.scala:19)
14:13:00.677 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
14:13:00.686 [dispatcher-event-loop-1] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5941 bytes)
14:13:00.691 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
14:13:00.984 [Executor task launch worker-0] INFO  o.a.s.s.e.datasources.FileScanRDD - Reading File path: file:///vagrant/src/test/resources/SimpleSQLContextData.json, range: 0-68, partition values: [empty row]
14:13:01.068 [Executor task launch worker-0] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 70.04924 ms
14:13:01.119 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned accumulator 176
14:13:01.174 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 1956 bytes result sent to driver
14:13:01.181 [task-result-getter-1] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 499 ms on localhost (1/1)
14:13:01.181 [task-result-getter-1] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
14:13:01.182 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (collect at SimpleHiveContext_SparkSession.scala:19) finished in 0.503 s
14:13:01.185 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - looking for newly runnable stages
14:13:01.186 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - running: Set()
14:13:01.188 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
14:13:01.189 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - failed: Set()
14:13:01.200 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[9] at collect at SimpleHiveContext_SparkSession.scala:19), which has no missing parents
14:13:01.221 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 8.3 KB, free 631.7 MB)
14:13:01.225 [dag-scheduler-event-loop] INFO  o.a.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.2 KB, free 631.7 MB)
14:13:01.226 [dispatcher-event-loop-1] INFO  o.a.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:47525 (size: 4.2 KB, free: 632.0 MB)
14:13:01.227 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
14:13:01.227 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at collect at SimpleHiveContext_SparkSession.scala:19)
14:13:01.228 [dag-scheduler-event-loop] INFO  o.a.s.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
14:13:01.246 [dispatcher-event-loop-0] INFO  o.a.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, NODE_LOCAL, 5342 bytes)
14:13:01.249 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
14:13:01.280 [Executor task launch worker-0] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
14:13:01.285 [Executor task launch worker-0] INFO  o.a.s.s.ShuffleBlockFetcherIterator - Started 0 remote fetches in 15 ms
14:13:01.320 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 1867 bytes result sent to driver
14:13:01.325 [dag-scheduler-event-loop] INFO  o.a.spark.scheduler.DAGScheduler - ResultStage 2 (collect at SimpleHiveContext_SparkSession.scala:19) finished in 0.089 s
14:13:01.325 [task-result-getter-2] INFO  o.a.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 89 ms on localhost (1/1)
14:13:01.328 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.spark.scheduler.DAGScheduler - Job 1 finished: collect at SimpleHiveContext_SparkSession.scala:19, took 0.776258 s
14:13:01.337 [task-result-getter-2] INFO  o.a.s.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
14:13:01.389 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  o.a.s.s.c.e.codegen.CodeGenerator - Code generated in 31.374227 ms
14:13:01.428 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  io.hydrosphere.mist.jobs.JobPy -  Started PythonGatewayServer on port 25333
14:13:01.892 [pool-4-thread-2-ScalaTest-running-JobTests] ERROR io.hydrosphere.mist.jobs.JobPy - null
14:13:01.895 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  io.hydrosphere.mist.jobs.JobPy -  Exiting due to broken pipe from Python driver
14:13:01.897 [pool-4-thread-2-ScalaTest-running-JobTests] ERROR io.hydrosphere.mist.jobs.JobPy - java.lang.Exception: Error in python code: null
java.lang.Exception: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:58) ~[classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$25.apply$mcV$sp(JobTests.scala:291) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$25.apply(JobTests.scala:282) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$25.apply(JobTests.scala:282) [test-classes/:na]
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:20) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at scala.collection.immutable.List.foreach(List.scala:381) [scala-library-2.11.8.jar:0.13.7]
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.run(Suite.scala:1424) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.org$scalatest$BeforeAndAfterAll$$super$run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:53) ~[classes/:na]
	... 67 common frames omitted
14:13:31.995 [pool-4-thread-2-ScalaTest-running-JobTests] ERROR io.hydrosphere.mist.jobs.JobJar - Test Error
java.lang.Exception: Test Error
	at TestError$.doStuff(TestError.scala:13) ~[na:na]
	at io.hydrosphere.mist.jobs.JobJarRun$class.run(JobJarRun_SparkSession.scala:30) ~[classes/:na]
	at io.hydrosphere.mist.jobs.JobJar.run(JobJar.scala:13) [classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$26.apply$mcV$sp(JobTests.scala:301) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$26.apply(JobTests.scala:297) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$26.apply(JobTests.scala:297) [test-classes/:na]
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:20) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at scala.collection.immutable.List.foreach(List.scala:381) [scala-library-2.11.8.jar:0.13.7]
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.run(Suite.scala:1424) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.org$scalatest$BeforeAndAfterAll$$super$run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
14:13:32.004 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  io.hydrosphere.mist.jobs.JobPy -  Started PythonGatewayServer on port 25333
14:13:32.483 [pool-4-thread-2-ScalaTest-running-JobTests] ERROR io.hydrosphere.mist.jobs.JobPy - null
14:13:32.485 [pool-4-thread-2-ScalaTest-running-JobTests] INFO  io.hydrosphere.mist.jobs.JobPy -  Exiting due to broken pipe from Python driver
14:13:32.488 [pool-4-thread-2-ScalaTest-running-JobTests] ERROR io.hydrosphere.mist.jobs.JobPy - java.lang.Exception: Error in python code: null
java.lang.Exception: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:58) ~[classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$27.apply$mcV$sp(JobTests.scala:311) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$27.apply(JobTests.scala:307) [test-classes/:na]
	at io.hydrosphere.mist.JobTests$$anonfun$27.apply(JobTests.scala:307) [test-classes/:na]
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:22) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Transformer.apply(Transformer.scala:20) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at scala.collection.immutable.List.foreach(List.scala:381) [scala-library-2.11.8.jar:0.13.7]
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.Suite$class.run(Suite.scala:1424) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.org$scalatest$BeforeAndAfterAll$$super$run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256) [scalatest_2.11-2.2.6.jar:na]
	at io.hydrosphere.mist.JobTests.run(JobTests.scala:119) [test-classes/:na]
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357) [scalatest_2.11-2.2.6.jar:na]
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502) [scalatest_2.11-2.2.6.jar:na]
	at sbt.TestRunner.runTest$1(TestFramework.scala:76) [testing-0.13.7.jar:0.13.7]
	at sbt.TestRunner.run(TestFramework.scala:85) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:185) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:202) [testing-0.13.7.jar:0.13.7]
	at sbt.TestFunction.apply(TestFramework.scala:207) [testing-0.13.7.jar:0.13.7]
	at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:239) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:245) [actions-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:44) [task-system-0.13.7.jar:0.13.7]
	at sbt.std.Transform$$anon$4.work(System.scala:63) [task-system-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17) [control-0.13.7.jar:0.13.7]
	at sbt.Execute.work(Execute.scala:235) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226) [tasks-0.13.7.jar:0.13.7]
	at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159) [tasks-0.13.7.jar:0.13.7]
	at sbt.CompletionService$$anon$2.call(CompletionService.scala:28) [tasks-0.13.7.jar:0.13.7]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.Exception: Error in python code: null
	at io.hydrosphere.mist.jobs.JobPy.run(JobPy.scala:53) ~[classes/:na]
	... 67 common frames omitted
14:13:32.524 [pool-4-thread-2] INFO  o.s.jetty.server.ServerConnector - Stopped ServerConnector@3fa7b8dc{HTTP/1.1}{0.0.0.0:4041}
14:13:32.528 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@41117d11{/stages/stage/kill,null,UNAVAILABLE}
14:13:32.529 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7953964b{/api,null,UNAVAILABLE}
14:13:32.533 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3e34d150{/,null,UNAVAILABLE}
14:13:32.538 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1ccf5df7{/static,null,UNAVAILABLE}
14:13:32.539 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@73896e6e{/executors/threadDump/json,null,UNAVAILABLE}
14:13:32.540 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4b61982d{/executors/threadDump,null,UNAVAILABLE}
14:13:32.546 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@56ee96e8{/executors/json,null,UNAVAILABLE}
14:13:32.548 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@388ae46c{/executors,null,UNAVAILABLE}
14:13:32.549 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7d28bf10{/environment/json,null,UNAVAILABLE}
14:13:32.552 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2e26769e{/environment,null,UNAVAILABLE}
14:13:32.553 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@43f5bffa{/storage/rdd/json,null,UNAVAILABLE}
14:13:32.554 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@67863ea{/storage/rdd,null,UNAVAILABLE}
14:13:32.555 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@491349db{/storage/json,null,UNAVAILABLE}
14:13:32.555 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@670147f5{/storage,null,UNAVAILABLE}
14:13:32.556 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@132acc4b{/stages/pool/json,null,UNAVAILABLE}
14:13:32.556 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@a18b784{/stages/pool,null,UNAVAILABLE}
14:13:32.557 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@774b2ee0{/stages/stage/json,null,UNAVAILABLE}
14:13:32.557 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@d2f2e1d{/stages/stage,null,UNAVAILABLE}
14:13:32.557 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1061395d{/stages/json,null,UNAVAILABLE}
14:13:32.558 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@45e71934{/stages,null,UNAVAILABLE}
14:13:32.558 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5b242747{/jobs/job/json,null,UNAVAILABLE}
14:13:32.558 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@25e7069a{/jobs/job,null,UNAVAILABLE}
14:13:32.559 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@250e0bae{/jobs/json,null,UNAVAILABLE}
14:13:32.560 [pool-4-thread-2] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4e5a10ff{/jobs,null,UNAVAILABLE}
14:13:32.562 [pool-4-thread-2] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://localhost:4041
14:13:32.596 [dispatcher-event-loop-1] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
14:13:32.607 [pool-4-thread-2] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
14:13:32.608 [pool-4-thread-2] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
14:13:32.610 [pool-4-thread-2] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
14:13:32.617 [dispatcher-event-loop-1] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
14:13:32.625 [pool-4-thread-2] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
[0m[[0minfo[0m] [0m[32mJobTests:[0m[0m
[0m[[0minfo[0m] [0m[32m- FileNotFoundException[0m[0m
[0m[[0minfo[0m] [0m[32m- Jar job[0m[0m
[0m[[0minfo[0m] [0m[33m- Jar job sql !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (JobTests.scala:160)[0m[0m
[0m[[0minfo[0m] [0m[32m- Jar job Hive[0m[0m
[0m[[0minfo[0m] [0m[33m- Py job !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (JobTests.scala:185)[0m[0m
[0m[[0minfo[0m] [0m[33m- Py job sql !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (JobTests.scala:196)[0m[0m
[0m[[0minfo[0m] [0m[32m- Py job hive[0m[0m
[0m[[0minfo[0m] [0m[33m- Jar job run !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (JobTests.scala:221)[0m[0m
[0m[[0minfo[0m] [0m[33m- Jar job sql run  !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (JobTests.scala:233)[0m[0m
[0m[[0minfo[0m] [0m[32m- Jar job hive run [0m[0m
[0m[[0minfo[0m] [0m[33m- Py job run !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (JobTests.scala:260)[0m[0m
[0m[[0minfo[0m] [0m[33m- Py job sql run !!! CANCELED !!![0m[0m
[0m[[0minfo[0m] [0m[33m  Can't run in Spark 2.0.0 (JobTests.scala:272)[0m[0m
[0m[[0minfo[0m] [0m[31m- Py job hive run  *** FAILED ***[0m[0m
[0m[[0minfo[0m] [0m[31m  The code passed to eventually never returned normally. Attempted 70 times over 30.049920245000003 seconds. Last failure message: Aborted did not equal Running. (JobTests.scala:292)[0m[0m
[0m[[0minfo[0m] [0m[32m- Jar job testerror run[0m[0m
[0m[[0minfo[0m] [0m[32m- Py job error [0m[0m
akka.stream.StreamTcpException: Tcp command [Connect(localhost/127.0.0.1:2004,None,List(),Some(10 seconds),true)] failed
java.lang.RuntimeException: Unexpected (early) disconnect
java.lang.RuntimeException: Unexpected (early) disconnect
java.lang.RuntimeException: Unexpected (early) disconnect
java.lang.RuntimeException: Unexpected (early) disconnect
[0m[[0minfo[0m] [0m[32mIntegrationTests:[0m[0m
[0m[[0minfo[0m] [0m[31m- HTTP bad request *** FAILED ***[0m[0m
[0m[[0minfo[0m] [0m[31m  akka.stream.StreamTcpException: Tcp command [Connect(localhost/127.0.0.1:2004,None,List(),Some(10 seconds),true)] failed[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m- HTTP bad patch *** FAILED ***[0m[0m
[0m[[0minfo[0m] [0m[31m  java.lang.RuntimeException: Unexpected (early) disconnect[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.handleDisconnect(PoolSlot.scala:193)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor$$anonfun$waitingForDemandFromConnection$1.applyOrElse(PoolSlot.scala:149)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.Actor$class.aroundReceive(Actor.scala:484)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorSubscriber$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorSubscriber$class.aroundReceive(ActorSubscriber.scala:201)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorPublisher$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorPublisher$class.aroundReceive(ActorPublisher.scala:309)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.invoke(ActorCell.scala:495)[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m  Cause: akka.stream.StreamTcpException: Tcp command [Connect(localhost/127.0.0.1:2004,None,List(),Some(10 seconds),true)] failed[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m- HTTP bad JSON *** FAILED ***[0m[0m
[0m[[0minfo[0m] [0m[31m  java.lang.RuntimeException: Unexpected (early) disconnect[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.handleDisconnect(PoolSlot.scala:193)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor$$anonfun$waitingForDemandFromConnection$1.applyOrElse(PoolSlot.scala:149)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.Actor$class.aroundReceive(Actor.scala:484)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorSubscriber$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorSubscriber$class.aroundReceive(ActorSubscriber.scala:201)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorPublisher$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorPublisher$class.aroundReceive(ActorPublisher.scala:309)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.invoke(ActorCell.scala:495)[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m  Cause: akka.stream.StreamTcpException: Tcp command [Connect(localhost/127.0.0.1:2004,None,List(),Some(10 seconds),true)] failed[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m- HTTP noDoStuff in jar *** FAILED ***[0m[0m
[0m[[0minfo[0m] [0m[31m  java.lang.RuntimeException: Unexpected (early) disconnect[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.handleDisconnect(PoolSlot.scala:193)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor$$anonfun$waitingForDemandFromConnection$1.applyOrElse(PoolSlot.scala:149)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.Actor$class.aroundReceive(Actor.scala:484)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorSubscriber$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorSubscriber$class.aroundReceive(ActorSubscriber.scala:201)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorPublisher$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorPublisher$class.aroundReceive(ActorPublisher.scala:309)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.invoke(ActorCell.scala:495)[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m  Cause: akka.stream.StreamTcpException: Tcp command [Connect(localhost/127.0.0.1:2004,None,List(),Some(10 seconds),true)] failed[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m- HTTP Spark Context jar *** FAILED ***[0m[0m
[0m[[0minfo[0m] [0m[31m  java.lang.RuntimeException: Unexpected (early) disconnect[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.handleDisconnect(PoolSlot.scala:193)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor$$anonfun$waitingForDemandFromConnection$1.applyOrElse(PoolSlot.scala:149)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.Actor$class.aroundReceive(Actor.scala:484)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorSubscriber$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorSubscriber$class.aroundReceive(ActorSubscriber.scala:201)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.akka$stream$actor$ActorPublisher$$super$aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.stream.actor.ActorPublisher$class.aroundReceive(ActorPublisher.scala:309)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.http.impl.engine.client.PoolSlot$SlotProcessor.aroundReceive(PoolSlot.scala:91)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)[0m[0m
[0m[[0minfo[0m] [0m[31m  at akka.actor.ActorCell.invoke(ActorCell.scala:495)[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[31m  Cause: akka.stream.StreamTcpException: Tcp command [Connect(localhost/127.0.0.1:2004,None,List(),Some(10 seconds),true)] failed[0m[0m
[0m[[0minfo[0m] [0m[31m  ...[0m[0m
[0m[[0minfo[0m] [0m[36mRun completed in 3 minutes, 20 seconds.[0m[0m
[0m[[0minfo[0m] [0m[36mTotal number of tests run: 49[0m[0m
[0m[[0minfo[0m] [0m[36mSuites: completed 8, aborted 0[0m[0m
[0m[[0minfo[0m] [0m[36mTests: succeeded 43, failed 6, canceled 16, ignored 0, pending 0[0m[0m
[0m[[0minfo[0m] [0m[31m*** 6 TESTS FAILED ***[0m[0m
[0m[[31merror[0m] [0mFailed tests:[0m
[0m[[31merror[0m] [0m	io.hydrosphere.mist.IntegrationTests[0m
[0m[[31merror[0m] [0m	io.hydrosphere.mist.JobTests[0m
[0m[[31merror[0m] [0m(mist/test:[31mtest[0m) sbt.TestsFailedException: Tests unsuccessful[0m
[0m[[31merror[0m] [0mTotal time: 328 s, completed Aug 18, 2016 2:14:02 PM[0m
14:14:02.172 [shutdownHook1] INFO  io.hydrosphere.mist.Master$ - Stopping all the contexts
14:14:02.175 [mist-akka.actor.default-dispatcher-2] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Marked address [akka.tcp://mist@127.0.0.1:39001] as [Leaving]
14:14:02.181 [Thread-7] INFO  org.apache.spark.SparkContext - Invoking stop() from shutdown hook
14:14:02.195 [Thread-7] INFO  o.s.jetty.server.ServerConnector - Stopped ServerConnector@7ae8d7cd{HTTP/1.1}{0.0.0.0:4040}
14:14:02.196 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@a165bd{/stages/stage/kill,null,UNAVAILABLE}
14:14:02.197 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@20f98840{/api,null,UNAVAILABLE}
14:14:02.198 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4ce55ae4{/,null,UNAVAILABLE}
14:14:02.199 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@62b542fe{/static,null,UNAVAILABLE}
14:14:02.200 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@38330973{/executors/threadDump/json,null,UNAVAILABLE}
14:14:02.200 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@dec3393{/executors/threadDump,null,UNAVAILABLE}
14:14:02.201 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@74ab4ab6{/executors/json,null,UNAVAILABLE}
14:14:02.202 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3c1cd80b{/executors,null,UNAVAILABLE}
14:14:02.202 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@24e2310c{/environment/json,null,UNAVAILABLE}
14:14:02.203 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@55359169{/environment,null,UNAVAILABLE}
14:14:02.204 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@222d5a44{/storage/rdd/json,null,UNAVAILABLE}
14:14:02.205 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@483a1b80{/storage/rdd,null,UNAVAILABLE}
14:14:02.207 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5684f44f{/storage/json,null,UNAVAILABLE}
14:14:02.208 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3869e0c8{/storage,null,UNAVAILABLE}
14:14:02.208 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5e569566{/stages/pool/json,null,UNAVAILABLE}
14:14:02.209 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4e8a054a{/stages/pool,null,UNAVAILABLE}
14:14:02.210 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5007c09a{/stages/stage/json,null,UNAVAILABLE}
14:14:02.210 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7fa33f43{/stages/stage,null,UNAVAILABLE}
14:14:02.213 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@186115a8{/stages/json,null,UNAVAILABLE}
14:14:02.217 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7732c1c{/stages,null,UNAVAILABLE}
14:14:02.217 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5554bd6{/jobs/job/json,null,UNAVAILABLE}
14:14:02.218 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5106d46b{/jobs/job,null,UNAVAILABLE}
14:14:02.218 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6d2e446e{/jobs/json,null,UNAVAILABLE}
14:14:02.219 [Thread-7] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@68fd5b23{/jobs,null,UNAVAILABLE}
14:14:02.221 [Thread-7] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://10.0.2.15:4040
14:14:02.231 [dispatcher-event-loop-0] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
14:14:02.238 [Thread-7] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
14:14:02.239 [Thread-7] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
14:14:02.239 [Thread-7] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
14:14:02.240 [dispatcher-event-loop-1] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
14:14:02.242 [Thread-7] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
14:14:02.248 [Thread-7] INFO  o.a.spark.util.ShutdownHookManager - Shutdown hook called
14:14:02.252 [Thread-7] INFO  o.a.spark.util.ShutdownHookManager - Deleting directory /tmp/spark-ecfb90b3-5ac5-44fc-a32a-9a213ce9448d
14:14:03.908 [mist-akka.actor.default-dispatcher-18] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:2551] - Leader is moving node [akka.tcp://mist@127.0.0.1:39001] to [Exiting]
14:14:04.091 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:39001] - Shutting down...
14:14:04.106 [mist-akka.actor.default-dispatcher-7] INFO  akka.cluster.Cluster(akka://mist) - Cluster Node [akka.tcp://mist@127.0.0.1:39001] - Successfully shut down
14:14:04.113 [mist-akka.actor.default-dispatcher-7] INFO  akka.actor.RepointableActorRef - Message [akka.cluster.ClusterEvent$MemberExited] from Actor[akka://mist/deadLetters] to Actor[akka://mist/system/clusterEventBusListener#1910267680] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14:14:04.113 [mist-akka.actor.default-dispatcher-7] INFO  akka.actor.RepointableActorRef - Message [akka.cluster.ClusterEvent$ReachabilityChanged] from Actor[akka://mist/deadLetters] to Actor[akka://mist/system/clusterEventBusListener#1910267680] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14:14:04.114 [mist-akka.actor.default-dispatcher-7] INFO  akka.actor.LocalActorRef - Message [akka.cluster.ClusterEvent$MemberExited] from Actor[akka://mist/deadLetters] to Actor[akka://mist/system/cluster/metrics#1731972998] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
14:14:04.115 [mist-akka.actor.default-dispatcher-18] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
14:14:04.115 [mist-akka.actor.default-dispatcher-18] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
14:14:04.123 [mist-akka.actor.default-dispatcher-18] INFO  a.r.RemoteActorRefProvider$RemotingTerminator - Remoting shut down.
[DEBUG] [08/18/2016 14:14:04.123] [mist-akka.actor.default-dispatcher-2] [EventStream] shutting down: StandardOutLogger started
[DEBUG] [08/18/2016 14:14:04.124] [mist-akka.actor.default-dispatcher-2] [EventStream] all default loggers stopped
